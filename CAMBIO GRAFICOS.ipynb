{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a87a0fd-861a-4c3f-bbe7-18bf811f830c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import plotly.express as px\n",
    "from supabase import create_client, Client\n",
    "import schedule\n",
    "import threading\n",
    "import time as tiempo\n",
    "import uuid\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import folium\n",
    "from streamlit_folium import st_folium\n",
    "\n",
    "st.set_page_config(page_title=\"Red Eléctrica\", layout=\"centered\")\n",
    "\n",
    "# Constantes de configuración de la API REE\n",
    "BASE_URL = \"https://apidatos.ree.es/es/datos/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "ENDPOINTS = {\n",
    "    \"demanda\": (\"demanda/evolucion\", \"hour\"),\n",
    "    \"balance\": (\"balance/balance-electrico\", \"day\"),\n",
    "    \"generacion\": (\"generacion/evolucion-renovable-no-renovable\", \"day\"),\n",
    "    \"intercambios\": (\"intercambios/todas-fronteras-programados\", \"day\"),\n",
    "    \"intercambios_baleares\": (\"intercambios/enlace-baleares\", \"day\"),\n",
    "}\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "\n",
    "# ------------------------------ UTILIDADES ------------------------------\n",
    "\n",
    "# Función para consultar un endpoint, según los parámetros dados, de la API de REE\n",
    "def get_data(endpoint_name, endpoint_info, params):\n",
    "    path, time_trunc = endpoint_info\n",
    "    params[\"time_trunc\"] = time_trunc\n",
    "    url = BASE_URL + path\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        # Si la búsqueda no fue bien, se devuelve una lista vacía\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        response_data = response.json()\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Verificamos si el item tiene \"content\" y asumimos que es una estructura compleja\n",
    "    for item in response_data.get(\"included\", []):\n",
    "        attrs = item.get(\"attributes\", {})\n",
    "        category = attrs.get(\"title\")\n",
    "\n",
    "        if \"content\" in attrs:\n",
    "            for sub in attrs[\"content\"]:\n",
    "                sub_attrs = sub.get(\"attributes\", {})\n",
    "                sub_cat = sub_attrs.get(\"title\")\n",
    "                for entry in sub_attrs.get(\"values\", []):\n",
    "                    entry[\"primary_category\"] = category\n",
    "                    entry[\"sub_category\"] = sub_cat\n",
    "                    data.append(entry)\n",
    "        else:\n",
    "            # Procesamos las estructuras más simples (demanda, generacion, intercambios_baleares), asumiendo que no hay subcategorías\n",
    "            for entry in attrs.get(\"values\", []):\n",
    "                entry[\"primary_category\"] = category\n",
    "                entry[\"sub_category\"] = None\n",
    "                data.append(entry)\n",
    "\n",
    "    return data\n",
    "\n",
    "# Función para insertar cada DataFrame en Supabase\n",
    "def insertar_en_supabase(nombre_tabla, df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Generamos IDs únicos\n",
    "    df[\"record_id\"] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "    # Convertimos fechas a string ISO\n",
    "    for col in [\"datetime\", \"extraction_timestamp\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    # Reemplazamos NaN por None\n",
    "    #df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    # Convertir a lista de diccionarios e insertar\n",
    "    data = df.to_dict(orient=\"records\")\n",
    "\n",
    "    try:\n",
    "        supabase.table(nombre_tabla).insert(data).execute()\n",
    "        print(f\"✅ Insertados en '{nombre_tabla}': {len(data)} filas\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al insertar en '{nombre_tabla}': {e}\")\n",
    "\n",
    "# ------------------------------ FUNCIONES DE DESCARGA ------------------------------\n",
    "# Función de extracción de datos de los últimos x años, devuelve DataFrame. Ejecutar una vez al inicio para poblar la base de datos.\n",
    "def get_data_for_last_x_years(num_years=3):\n",
    "    all_dfs = []\n",
    "    current_date = datetime.now()\n",
    "    # Calculamos el año de inicio a partir del año actual\n",
    "    start_year_limit = current_date.year - num_years\n",
    "\n",
    "    # Iteramos sobre cada año y mes\n",
    "    for year in range(start_year_limit, current_date.year + 1):\n",
    "        for month in range(1, 13):\n",
    "            # Si el mes es mayor al mes actual y el año es el actual, lo saltamos\n",
    "            month_start = datetime(year, month, 1)\n",
    "            if month_start > current_date:\n",
    "                continue\n",
    "            # Calculamos el final del mes, asegurándonos de no exceder la fecha actual\n",
    "            month_end = (month_start + timedelta(days=32)).replace(day=1) - timedelta(minutes=1)\n",
    "            end_date_for_request = min(month_end, current_date)\n",
    "\n",
    "            monthly_data = []  # para acumular todos los dfs del mes\n",
    "\n",
    "            # Iteramos sobre cada endpoint y sacamos los datos\n",
    "            for name, (path, granularity) in ENDPOINTS.items():\n",
    "                params = {\n",
    "                    \"start_date\": month_start.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "                    \"end_date\": end_date_for_request.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "                    \"geo_trunc\": \"electric_system\",\n",
    "                    \"geo_limit\": \"peninsular\",\n",
    "                    \"geo_ids\": \"8741\"\n",
    "                }\n",
    "\n",
    "                data = get_data(name, (path, granularity), params)\n",
    "\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    #Lidiamos con problemas de zona horaria en la columna \"datetime\"\n",
    "                    try:\n",
    "                        df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    # Obtenemos nuevas columnas y las reordenamos\n",
    "                    df['year'] = df['datetime'].dt.year\n",
    "                    df['month'] = df['datetime'].dt.month\n",
    "                    df['day'] = df['datetime'].dt.day\n",
    "                    df['hour'] = df['datetime'].dt.hour\n",
    "                    df['extraction_timestamp'] = datetime.utcnow()\n",
    "                    df['endpoint'] = name\n",
    "                    df['record_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "                    df = df[['record_id', 'value', 'percentage', 'datetime',\n",
    "                             'primary_category', 'sub_category', 'year', 'month',\n",
    "                             'day', 'hour', 'endpoint', 'extraction_timestamp']]\n",
    "\n",
    "                    monthly_data.append(df)\n",
    "                    tiempo.sleep(1)\n",
    "\n",
    "            # Generamos los dataframes individuales\n",
    "            if monthly_data:\n",
    "                df_nuevo = pd.concat(monthly_data, ignore_index=True)\n",
    "                all_dfs.append(df_nuevo)\n",
    "\n",
    "                tablas_dfs = {\n",
    "                    \"demanda\": df_nuevo[df_nuevo[\"endpoint\"] == \"demanda\"].drop(columns=[\"endpoint\", \"sub_category\"], errors='ignore'),\n",
    "                    \"balance\": df_nuevo[df_nuevo[\"endpoint\"] == \"balance\"].drop(columns=[\"endpoint\"], errors='ignore'),\n",
    "                    \"generacion\": df_nuevo[df_nuevo[\"endpoint\"] == \"generacion\"].drop(columns=[\"endpoint\", \"sub_category\"], errors='ignore'),\n",
    "                    \"intercambios\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios\"].drop(columns=[\"endpoint\"], errors='ignore'),\n",
    "                    \"intercambios_baleares\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios_baleares\"].drop(columns=[\"endpoint\", \"sub_category\"], errors='ignore'),\n",
    "                }\n",
    "\n",
    "                for tabla, df_tabla in tablas_dfs.items():\n",
    "                    if not df_tabla.empty:\n",
    "                        insertar_en_supabase(tabla, df_tabla)\n",
    "\n",
    "    return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "\n",
    "# Función para actualizar los datos desde la API cada 24 horas\n",
    "def actualizar_datos_desde_api():\n",
    "    print(f\"[{datetime.now()}] ⏳ Ejecutando extracción desde API...\")\n",
    "    current_date = datetime.now()\n",
    "    start_date = current_date - timedelta(days=1)\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for name, (path, granularity) in ENDPOINTS.items():\n",
    "        params = {\n",
    "            \"start_date\": start_date.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "            \"end_date\": current_date.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "            \"geo_trunc\": \"electric_system\",\n",
    "            \"geo_limit\": \"peninsular\",\n",
    "            \"geo_ids\": \"8741\"\n",
    "        }\n",
    "\n",
    "        datos = get_data(name, (path, granularity), params)\n",
    "\n",
    "        if datos:\n",
    "            df = pd.DataFrame(datos)\n",
    "            try:\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            df['year'] = df['datetime'].dt.year\n",
    "            df['month'] = df['datetime'].dt.month\n",
    "            df['day'] = df['datetime'].dt.day\n",
    "            df['hour'] = df['datetime'].dt.hour\n",
    "            df['extraction_timestamp'] = datetime.utcnow()\n",
    "            df['endpoint'] = name\n",
    "            df['record_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "            df = df[['record_id', 'value', 'percentage', 'datetime',\n",
    "                     'primary_category', 'sub_category', 'year', 'month',\n",
    "                     'day', 'hour', 'endpoint', 'extraction_timestamp']]\n",
    "\n",
    "            all_dfs.append(df)\n",
    "            tiempo.sleep(1)\n",
    "        else:\n",
    "            print(f\"⚠️ No se obtuvieron datos de '{name}'\")\n",
    "\n",
    "    if all_dfs:\n",
    "        df_nuevo = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "        tablas_dfs = {\n",
    "            \"demanda\": df_nuevo[df_nuevo[\"endpoint\"] == \"demanda\"].drop(columns=[\"endpoint\", \"sub_category\"]),\n",
    "            \"balance\": df_nuevo[df_nuevo[\"endpoint\"] == \"balance\"].drop(columns=[\"endpoint\"]),\n",
    "            \"generacion\": df_nuevo[df_nuevo[\"endpoint\"] == \"generacion\"].drop(columns=[\"endpoint\", \"sub_category\"]),\n",
    "            \"intercambios\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios\"].drop(columns=[\"endpoint\"]),\n",
    "            \"intercambios_baleares\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios_baleares\"].drop(columns=[\"endpoint\", \"sub_category\"]),\n",
    "        }\n",
    "\n",
    "        for tabla, df in tablas_dfs.items():\n",
    "            if not df.empty:\n",
    "                insertar_en_supabase(tabla, df)\n",
    "\n",
    "# Programador para actualizar datos desde la API cada 24 horas\n",
    "def iniciar_programador_api():\n",
    "    schedule.every(24).hours.do(actualizar_datos_desde_api)\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        tiempo.sleep(60)\n",
    "\n",
    "#threading.Thread(target=iniciar_programador_api, daemon=True).start()\n",
    "\n",
    "# ------------------------------ CONSULTA SUPABASE ------------------------------\n",
    "\n",
    "def get_data_from_supabase(table_name, start_date, end_date, page_size=1000):\n",
    "    end_date += timedelta(days=1)\n",
    "    start_iso = start_date.isoformat()\n",
    "    end_iso = end_date.isoformat()\n",
    "\n",
    "    all_data = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        response = (\n",
    "            supabase.table(table_name)\n",
    "            .select(\"*\")\n",
    "            .gte(\"datetime\", start_iso)\n",
    "            .lte(\"datetime\", end_iso)\n",
    "            .range(offset, offset + page_size - 1)\n",
    "            .execute()\n",
    "        )\n",
    "        data = response.data\n",
    "        if not data:\n",
    "            break\n",
    "        all_data.extend(data)\n",
    "        offset += page_size\n",
    "        if len(data) < page_size:\n",
    "            break\n",
    "\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    return df\n",
    "\n",
    "# ------------------------------ INTERFAZ ------------------------------\n",
    "\n",
    "def main():\n",
    "    st.title(\"Análisis de la Red Eléctrica Española\")\n",
    "\n",
    "    tab1, tab2, tab3, tab4 = st.tabs([\"Descripción\", \"Consulta de datos\", \"Visualización\", \"Extras\"])\n",
    "\n",
    "    with tab2:  # Mueve el contexto de la tab2 aquí para que `modo` se defina antes de usarse en session_state\n",
    "        st.subheader(\"Consulta de datos\")\n",
    "\n",
    "        modo = st.radio(\"Tipo de consulta:\", [\"Últimos días\", \"Año específico\", \"Histórico\"], horizontal=True,\n",
    "                        key=\"query_mode_radio\")\n",
    "        st.session_state[\"modo_seleccionado\"] = modo  # Guardar el modo en session_state\n",
    "\n",
    "        tabla = st.selectbox(\"Selecciona la tabla:\", list(ENDPOINTS.keys()), key=\"query_table_select\")\n",
    "\n",
    "        df = pd.DataFrame()  # Inicializamos el DataFrame para evitar errores\n",
    "\n",
    "        if modo == \"Últimos días\":\n",
    "            dias = st.selectbox(\"¿Cuántos días atrás?\", [7, 14, 30], key=\"query_days_select\")\n",
    "            end_date_query = datetime.now(timezone.utc)\n",
    "            start_date_query = end_date_query - timedelta(days=dias)\n",
    "            st.session_state[\"selected_year_for_viz\"] = None  # Reset year if not in \"Año específico\" mode\n",
    "        elif modo == \"Año específico\":\n",
    "            current_year = datetime.now().year\n",
    "            # Se usa `key` para que el selectbox mantenga su estado\n",
    "            año = st.selectbox(\"Selecciona el año:\", [current_year - i for i in range(3)], index=0,\n",
    "                               key=\"query_year_select\")\n",
    "            st.session_state[\"selected_year_for_viz\"] = año  # Store the selected year\n",
    "            start_date_query = datetime(año, 1, 1, tzinfo=timezone.utc)\n",
    "            end_date_query = datetime(año, 12, 31, 23, 59, 59, 999999, tzinfo=timezone.utc)\n",
    "        elif modo == \"Histórico\":\n",
    "            # Si quieres cargar datos históricos de muchos años, ten cuidado con el rendimiento\n",
    "            start_date_query = datetime(2022, 1, 1, 0, 0, 0, tzinfo=timezone.utc)  # Ejemplo de fecha inicial\n",
    "            end_date_query = datetime.now(timezone.utc)\n",
    "            st.session_state[\"selected_year_for_viz\"] = None  # Reset year if not in \"Año específico\" mode\n",
    "\n",
    "        with st.spinner(\"Consultando Supabase...\"):\n",
    "            st.session_state[\"tabla_seleccionada_en_tab2\"] = tabla\n",
    "            df = get_data_from_supabase(tabla, start_date_query, end_date_query)\n",
    "\n",
    "        # Mostrar resultados después de la consulta de cualquier modo\n",
    "        if not df.empty:\n",
    "            st.session_state[\"ree_data\"] = df\n",
    "            st.session_state[\"tabla\"] = tabla  # Esto es redundante si usas tabla_seleccionada_en_tab2, pero lo mantengo por seguridad\n",
    "            st.write(f\"Datos recuperados: {len(df)} filas\")\n",
    "            st.write(\"Último dato:\", df['datetime'].max())\n",
    "            st.success(\"Datos cargados correctamente desde Supabase.\")\n",
    "        else:\n",
    "            st.warning(\"No se encontraron datos para ese período.\")\n",
    "\n",
    "    with tab1:  # Reordeno esto para que la tab2 se cargue primero y defina el estado\n",
    "        st.subheader(\"¿Qué es esta app?\")\n",
    "        st.markdown(\"\"\"Este proyecto explora los datos públicos de la **Red Eléctrica de España (REE)** a través de su API.\n",
    "Se analizan aspectos como:\n",
    "\n",
    "- La **demanda eléctrica** por hora.\n",
    "- El **balance eléctrico** por día.\n",
    "- La **generación** por mes.\n",
    "- Los **intercambios programados** con otros países.\n",
    "\n",
    "Estos datos permiten visualizar la evolución energética de España y generar análisis útiles para planificación y sostenibilidad.\"\"\")\n",
    "\n",
    "    with tab3:\n",
    "        st.subheader(\"Visualización\")\n",
    "        if \"ree_data\" in st.session_state and not st.session_state[\"ree_data\"].empty:\n",
    "            # Recuperamos el DataFrame principal de la sesión para el primer gráfico\n",
    "            df = st.session_state[\"ree_data\"]\n",
    "            tabla = st.session_state[\"tabla_seleccionada_en_tab2\"]  # Usamos la tabla seleccionada en tab2\n",
    "            modo_actual = st.session_state.get(\"modo_seleccionado\", \"Últimos días\")  # Obtener el modo\n",
    "\n",
    "            if tabla == \"demanda\":\n",
    "                fig = px.line(df, x=\"datetime\", y=\"value\", title=\"Demanda Eléctrica\", labels={\"value\": \"MW\"}) #CAMBIADOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "                # --- Nuevo gráfico: Histograma de demanda con outliers para año específico ---\n",
    "                if modo_actual == \"Año específico\":\n",
    "                    año_seleccionado = st.session_state.get(\"selected_year_for_viz\")\n",
    "                    if año_seleccionado is None:\n",
    "                        st.warning(\n",
    "                            \"Por favor, selecciona un año en la pestaña 'Consulta de datos' para ver el histograma de demanda.\")\n",
    "                    else:\n",
    "                        st.subheader(f\"Distribución de Demanda y Valores Atípicos para el año {año_seleccionado}\")\n",
    "\n",
    "                        # Filtra el DataFrame para el año seleccionado (df ya debe estar filtrado por el año, pero esto es por seguridad)\n",
    "                        df_año = df[df['year'] == año_seleccionado].copy()\n",
    "\n",
    "                        if not df_año.empty:\n",
    "                            # Calcular Q1, Q3 y el IQR para la columna 'value' (demanda)\n",
    "                            Q1 = df_año['value'].quantile(0.25)\n",
    "                            Q3 = df_año['value'].quantile(0.75)\n",
    "                            IQR = Q3 - Q1\n",
    "\n",
    "                            # Calcular los límites de Tukey's Fence\n",
    "                            lower_bound = Q1 - 1.5 * IQR\n",
    "                            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                            # Identificar valores atípicos\n",
    "                            df_año['is_outlier'] = 'Normal'\n",
    "                            df_año.loc[df_año['value'] < lower_bound, 'is_outlier'] = 'Atípico (bajo)'\n",
    "                            df_año.loc[df_año['value'] > upper_bound, 'is_outlier'] = 'Atípico (alto)'\n",
    "\n",
    "                            # Crear el histograma\n",
    "                            fig_hist_outliers = px.histogram(\n",
    "                                df_año,\n",
    "                                x=\"value\",\n",
    "                                color=\"is_outlier\",\n",
    "                                title=f\"Distribución Horaria de Demanda para {año_seleccionado}\",\n",
    "                                labels={\"value\": \"Demanda (MW)\", \"is_outlier\": \"Tipo de Valor\"},\n",
    "                                category_orders={\"is_outlier\": [\"Atípico (bajo)\", \"Normal\", \"Atípico (alto)\"]},\n",
    "                                color_discrete_map={'Normal': 'skyblue', 'Atípico (bajo)': 'orange',\n",
    "                                                    'Atípico (alto)': 'red'},\n",
    "                                nbins=50  # Ajusta el número de bins según la granularidad deseada\n",
    "                            )\n",
    "                            fig_hist_outliers.update_layout(bargap=0.1)  # Espacio entre barras\n",
    "                            st.plotly_chart(fig_hist_outliers, use_container_width=True)\n",
    "\n",
    "                            # Mostrar información sobre outliers\n",
    "                            num_outliers_low = (df_año['is_outlier'] == 'Atípico (bajo)').sum()\n",
    "                            num_outliers_high = (df_año['is_outlier'] == 'Atípico (alto)').sum()\n",
    "\n",
    "                            if num_outliers_low > 0 or num_outliers_high > 0:\n",
    "                                st.warning(\n",
    "                                    f\"Se han identificado {num_outliers_low} valores atípicos por debajo y {num_outliers_high} por encima (método IQR).\")\n",
    "                                st.info(f\"Rango normal de demanda (IQR): {lower_bound:.2f} MW - {upper_bound:.2f} MW\")\n",
    "                            else:\n",
    "                                st.info(\n",
    "                                    \"No se han identificado valores atípicos de demanda significativos (método IQR).\")\n",
    "                        else:\n",
    "                            st.warning(\n",
    "                                f\"No hay datos de demanda para el año {año_seleccionado} para generar el histograma.\")\n",
    "\n",
    "                # --- Condición para mostrar la comparativa de años (mantenida de antes) ---\n",
    "                if modo_actual == \"Histórico\":\n",
    "                    st.subheader(\"Comparativa de Demanda entre años\")\n",
    "\n",
    "                    # Definir los dos años específicos para la comparación: el año pasado y el anterior\n",
    "                    current_year = datetime.now().year\n",
    "\n",
    "                    # Los años que queremos comparar: el año anterior al actual y el año anterior a ese\n",
    "                    target_years_for_comparison = [current_year - 2, current_year - 1]\n",
    "\n",
    "                    # Obtener todos los años disponibles en el DataFrame del modo histórico\n",
    "                    all_available_years_in_df = sorted(list(df['year'].unique()))\n",
    "\n",
    "                    # Filtrar solo los años que queremos comparar y que realmente están disponibles en el df\n",
    "                    years_for_comparison = [\n",
    "                        year for year in target_years_for_comparison\n",
    "                        if year in all_available_years_in_df\n",
    "                    ]\n",
    "\n",
    "                    if len(years_for_comparison) == 2:\n",
    "                        # Asegurarse de que están en el orden deseado (ej. [2023, 2024])\n",
    "                        years_for_comparison.sort()\n",
    "                    elif len(years_for_comparison) == 1:\n",
    "                        st.info(\n",
    "                            f\"Solo se encontró un año de los deseados ({years_for_comparison[0]}) en modo histórico para la comparación. Se necesitan ambos años ({target_years_for_comparison[0]} y {target_years_for_comparison[1]}).\")\n",
    "                        years_for_comparison = []  # Vaciar para no intentar graficar\n",
    "                    else:  # len(years_for_comparison) == 0\n",
    "                        st.info(\n",
    "                            f\"No se encontraron datos para los años {target_years_for_comparison[0]} y {target_years_for_comparison[1]} en modo histórico para la comparación.\")\n",
    "                        years_for_comparison = []  # Vaciar para no intentar graficar\n",
    "\n",
    "                    if years_for_comparison:  # Solo procede si tenemos al menos dos años para comparar\n",
    "                        df_comparison_demanda = df.copy()  # Usamos el df ya cargado en modo histórico\n",
    "\n",
    "                        # Nos aseguramos de que solo tengamos los años que queremos comparar\n",
    "                        df_filtered_comparison = df_comparison_demanda[\n",
    "                            df_comparison_demanda['year'].isin(years_for_comparison)].copy()\n",
    "\n",
    "                        # Convertimos la columna 'datetime' a una fecha sin el año, para comparar día a día\n",
    "                        # Esta 'sort_key' se usa para el gráfico horario\n",
    "                        df_filtered_comparison['sort_key'] = df_filtered_comparison['datetime'].apply(\n",
    "                            lambda dt: dt.replace(year=2000)  # Usar un año base para ordenar correctamente\n",
    "                        )\n",
    "                        df_filtered_comparison = df_filtered_comparison.sort_values('sort_key')\n",
    "\n",
    "                        # --- Gráfico de Demanda Horaria General Comparativa ---\n",
    "                        fig_comp_hourly = px.line(\n",
    "                            df_filtered_comparison,\n",
    "                            x=\"sort_key\",  # Usamos la 'sort_key' que es datetime\n",
    "                            y=\"value\",\n",
    "                            color=\"year\",\n",
    "                            title=\"Demanda Horaria - Comparativa\",\n",
    "                            labels={\"sort_key\": \"Mes y Día\", \"value\": \"Demanda (MW)\", \"year\": \"Año\"},\n",
    "                            hover_data={\"year\": True, \"datetime\": \"|%Y-%m-%d %H:%M\"}\n",
    "                        )\n",
    "                        fig_comp_hourly.update_xaxes(tickformat=\"%b %d\")  # Formato para mostrar Mes y Día en el eje X\n",
    "                        st.plotly_chart(fig_comp_hourly, use_container_width=True)\n",
    "\n",
    "                        # --- Gráficos de Comparación de Métricas Diarias (Media, Mediana, Mínima, Máxima) ---\n",
    "                        # Agrupar por 'year' y 'month-day' para obtener las métricas diarias para cada año\n",
    "                        metrics_comp = df_filtered_comparison.groupby(\n",
    "                            ['year', df_filtered_comparison['datetime'].dt.strftime('%m-%d')])['value'].agg(\n",
    "                            ['mean', 'median', 'min', 'max']).reset_index()\n",
    "                        metrics_comp.columns = ['year', 'month_day', 'mean', 'median', 'min', 'max']\n",
    "\n",
    "                        # La corrección para el ValueError: day is out of range for month está aquí\n",
    "                        metrics_comp['sort_key'] = pd.to_datetime('2000-' + metrics_comp['month_day'],\n",
    "                                                                  format='%Y-%m-%d')\n",
    "                        metrics_comp = metrics_comp.sort_values('sort_key')\n",
    "\n",
    "                        metric_names = {\n",
    "                            'mean': 'Media diaria de demanda',\n",
    "                            'median': 'Mediana diaria de demanda',\n",
    "                            'min': 'Mínima diaria de demanda',\n",
    "                            'max': 'Máxima diaria de demanda',\n",
    "                        }\n",
    "\n",
    "                        for metric in ['mean', 'median', 'min', 'max']:\n",
    "                            fig = px.line(\n",
    "                                metrics_comp,\n",
    "                                x=\"sort_key\",  # <--- CAMBIO CLAVE: Usar 'sort_key' (tipo datetime) para el eje X\n",
    "                                y=metric,\n",
    "                                color=\"year\",\n",
    "                                title=metric_names[metric],\n",
    "                                labels={\"sort_key\": \"Fecha (Mes-Día)\", metric: \"Demanda (MW)\", \"year\": \"Año\"},\n",
    "                                # <--- CAMBIO EN ETIQUETA\n",
    "                            )\n",
    "                            fig.update_xaxes(tickformat=\"%b %d\")  # Formato para mostrar solo Mes y Día\n",
    "                            # Si las líneas siguen entrecortadas, considera añadir `connectgaps=True`\n",
    "                            # fig.update_traces(connectgaps=True)\n",
    "                            st.plotly_chart(fig, use_container_width=True)\n",
    "                    else:\n",
    "                        st.warning(f\"No hay suficientes datos de Demanda disponibles para la comparación.\")\n",
    "\n",
    "                    # --- Gráfico de Identificación de años outliers (mantenida de antes) ---\n",
    "                    st.subheader(\"Identificación de Años Outliers (Demanda Anual Total)\")\n",
    "\n",
    "                    # Asegurarse de que el df tiene la columna 'year'\n",
    "                    if 'year' not in df.columns:\n",
    "                        df['year'] = df['datetime'].dt.year\n",
    "\n",
    "                    # Agrupar por año para obtener la demanda total anual\n",
    "                    df_annual_summary = df.groupby('year')['value'].sum().reset_index()\n",
    "                    df_annual_summary.rename(columns={'value': 'total_demand_MW'}, inplace=True)\n",
    "\n",
    "                    if not df_annual_summary.empty and len(df_annual_summary) > 1:\n",
    "                        # Calcular Q1, Q3 y el IQR\n",
    "                        Q1 = df_annual_summary['total_demand_MW'].quantile(0.25)\n",
    "                        Q3 = df_annual_summary['total_demand_MW'].quantile(0.75)\n",
    "                        IQR = Q3 - Q1\n",
    "\n",
    "                        # Calcular los límites para los outliers\n",
    "                        lower_bound = Q1 - 1.5 * IQR\n",
    "                        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                        # Identificar los años que son outliers\n",
    "                        df_annual_summary['is_outlier'] = (\n",
    "                                (df_annual_summary['total_demand_MW'] < lower_bound) |\n",
    "                                (df_annual_summary['total_demand_MW'] > upper_bound)\n",
    "                        )\n",
    "\n",
    "                        # Crear el gráfico de barras\n",
    "                        fig_outliers = px.bar(\n",
    "                            df_annual_summary,\n",
    "                            x='year',\n",
    "                            y='total_demand_MW',\n",
    "                            color='is_outlier',  # Colorear las barras si son outliers\n",
    "                            title='Demanda Total Anual y Años Outlier',\n",
    "                            labels={'total_demand_MW': 'Demanda Total Anual (MW)', 'year': 'Año',\n",
    "                                    'is_outlier': 'Es Outlier'},\n",
    "                            color_discrete_map={False: 'skyblue', True: 'red'}  # Definir colores\n",
    "                        )\n",
    "\n",
    "                        st.plotly_chart(fig_outliers, use_container_width=True)\n",
    "\n",
    "                        # Mostrar los años identificados como outliers\n",
    "                        outlier_years = df_annual_summary[df_annual_summary['is_outlier']]['year'].tolist()\n",
    "                        if outlier_years:\n",
    "                            st.warning(\n",
    "                                f\"Se han identificado los siguientes años como outliers: {', '.join(map(str, outlier_years))}\")\n",
    "                        else:\n",
    "                            st.info(\"No se han identificado años outliers significativos (según el método IQR).\")\n",
    "                    elif not df_annual_summary.empty and len(df_annual_summary) <= 1:\n",
    "                        st.info(\"Se necesitan al menos 2 años de datos para calcular outliers de demanda anual.\")\n",
    "                    else:\n",
    "                        st.warning(\"No hay datos anuales disponibles para calcular outliers.\")\n",
    "                # El siguiente 'else' se aplica si modo_actual NO es \"Histórico\"\n",
    "                elif modo_actual != \"Histórico\" and modo_actual != \"Año específico\":\n",
    "                    st.info(\n",
    "                        \"Selecciona el modo 'Histórico' para ver la comparativa de años y la identificación de outliers anuales, o 'Año específico' para el histograma de demanda con outliers.\")\n",
    "\n",
    "            elif tabla == \"balance\":\n",
    "                fig = px.bar(df, x=\"datetime\", y=\"value\", color=\"primary_category\", barmode=\"group\", title=\"Balance Eléctrico\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "            elif tabla == \"generacion\":\n",
    "                df['date'] = df['datetime'].dt.date  # Para reducir a nivel diario (si no lo tienes)\n",
    "\n",
    "                df_grouped = df.groupby(['date', 'primary_category'])['value'].sum().reset_index()\n",
    "\n",
    "                fig = px.line(\n",
    "                    df_grouped,\n",
    "                    x=\"date\",\n",
    "                    y=\"value\",\n",
    "                    color=\"primary_category\",\n",
    "                    title=\"Generación diaria agregada por tipo\"\n",
    "                )\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "            elif tabla == \"intercambios\":\n",
    "                st.subheader(\"Mapa Coroplético de Intercambios Eléctricos\")\n",
    "\n",
    "                # Agrupamos y renombramos columnas\n",
    "                df_map = df.groupby(\"primary_category\")[\"value\"].sum().reset_index()\n",
    "                df_map.columns = [\"pais_original\", \"Total\"]\n",
    "\n",
    "                # Mapeo de nombres a inglés (para coincidir con el GeoJSON)\n",
    "                nombre_map = {\n",
    "                    \"francia\": \"France\",\n",
    "                    \"portugal\": \"Portugal\",\n",
    "                    \"andorra\": \"Andorra\",\n",
    "                    \"marruecos\": \"Morocco\"\n",
    "                }\n",
    "                df_map[\"Country\"] = df_map[\"pais_original\"].map(nombre_map)\n",
    "\n",
    "                df_map = df_map.dropna(subset=[\"Country\"])\n",
    "\n",
    "                # Cargar el archivo GeoJSON\n",
    "                with open(\"world_countries_with_andorra.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    world_geo = json.load(f)\n",
    "\n",
    "                # Crear el mapa\n",
    "                world_map = folium.Map(location=[40, 20], zoom_start=4)\n",
    "\n",
    "                # CAMBIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO: Mapa interactivo con leyenda y tooltips\n",
    "                choropleth = folium.Choropleth(\n",
    "                    geo_data=world_geo,\n",
    "                    data=df_map,\n",
    "                    columns=[\"Country\", \"Total\"],\n",
    "                    key_on=\"feature.properties.name\",\n",
    "                    fill_color=\"RdBu\",\n",
    "                    fill_opacity=0.7,\n",
    "                    line_opacity=0.2,\n",
    "                    legend_name=\"Saldo neto de energía (MWh)\"\n",
    "                )\n",
    "                choropleth.add_to(world_map)\n",
    "\n",
    "                # Tooltips al pasar el cursor\n",
    "                folium.GeoJsonTooltip(\n",
    "                    fields=[\"name\"],\n",
    "                    aliases=[\"País:\"],\n",
    "                    localize=True,\n",
    "                    sticky=False,\n",
    "                    labels=True,\n",
    "                    style=(\"background-color: white; color: #333; font-size: 12px; padding: 5px;\"),\n",
    "                ).add_to(choropleth.geojson)\n",
    "\n",
    "                # Etiquetas con los valores numéricos\n",
    "                for _, row in df_map.iterrows():\n",
    "                    country = row[\"Country\"]\n",
    "                    total = row[\"Total\"]\n",
    "                    tooltip_text = f\"{country}: {total:.2f} MWh\"\n",
    "                    for feature in world_geo[\"features\"]:\n",
    "                        if feature[\"properties\"][\"name\"] == country:\n",
    "                            coords = feature[\"geometry\"][\"coordinates\"]\n",
    "                            if feature[\"geometry\"][\"type\"] == \"Polygon\":\n",
    "                                lon, lat = coords[0][0]\n",
    "                            elif feature[\"geometry\"][\"type\"] == \"MultiPolygon\":\n",
    "                                lon, lat = coords[0][0][0]\n",
    "                            folium.Marker(\n",
    "                                location=[lat, lon],\n",
    "                                icon=folium.DivIcon(html=f\"\"\"<div style=\"font-size: 10pt\">{tooltip_text}</div>\"\"\")\n",
    "                            ).add_to(world_map)\n",
    "                            break\n",
    "\n",
    "\n",
    "                # Mostrar en Streamlit\n",
    "                st_folium(world_map, width=1285)\n",
    "\n",
    "            elif tabla == \"intercambios_baleares\":\n",
    "                # Filtramos las dos categorías\n",
    "                df_ib = df[df['primary_category'].isin(['Entradas', 'Salidas'])].copy()\n",
    "\n",
    "                # Agregamos por fecha para evitar múltiples por hora si fuera el caso\n",
    "                df_ib_grouped = df_ib.groupby(['datetime', 'primary_category'])['value'].sum().reset_index()\n",
    "\n",
    "                df_ib_grouped['value'] = df_ib_grouped['value'].abs()\n",
    "\n",
    "                fig = px.area(\n",
    "                    df_ib_grouped,\n",
    "                    x=\"datetime\",\n",
    "                    y=\"value\",\n",
    "                    color=\"primary_category\",\n",
    "                    labels={\"value\": \"Energía (MWh)\", \"datetime\": \"Fecha\"},\n",
    "                    title=\"Intercambios con Baleares - Área Apilada (Magnitud)\"\n",
    "                )\n",
    "\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "            else:\n",
    "                fig = px.line(df, x=\"datetime\", y=\"value\", title=\"Visualización\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "\n",
    "            with st.expander(\"Ver datos en tabla\"):\n",
    "                st.dataframe(df, use_container_width=True)\n",
    "        else:\n",
    "            st.info(\"Consulta primero los datos desde la pestaña anterior.\")\n",
    "\n",
    "    with tab4:\n",
    "        if tabla == \"demanda\":\n",
    "\n",
    "            # --- HEATMAP ---\n",
    "            df_heatmap = df.copy()\n",
    "            df_heatmap['weekday'] = df_heatmap['datetime'].dt.day_name()\n",
    "            df_heatmap['hour'] = df_heatmap['datetime'].dt.hour\n",
    "\n",
    "            days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "            heatmap_data = (\n",
    "                df_heatmap.groupby(['weekday', 'hour'])['value']\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .pivot(index='weekday', columns='hour', values='value')\n",
    "                .reindex(days_order)\n",
    "            )\n",
    "\n",
    "            fig1 = px.imshow(\n",
    "                heatmap_data,\n",
    "                labels=dict(x=\"Hora del día\", y=\"Día de la semana\", color=\"Demanda promedio (MW)\"),\n",
    "                x=heatmap_data.columns,\n",
    "                y=heatmap_data.index,\n",
    "                color_continuous_scale=\"YlGnBu\",\n",
    "                aspect=\"auto\",\n",
    "            )\n",
    "            fig1.update_layout(title=\"Demanda promedio por día y hora\")\n",
    "\n",
    "\n",
    "            st.plotly_chart(fig1, use_container_width=True)\n",
    "\n",
    "            # --- BOXPLOT ---\n",
    "            df_box = df.copy()\n",
    "\n",
    "            df_box[\"month\"] = df_box[\"datetime\"].dt.month\n",
    "\n",
    "            fig2 = px.box(\n",
    "                df_box,\n",
    "                x=\"month\",\n",
    "                y=\"value\",\n",
    "                title=\"Distribución de Demanda por mes\",\n",
    "                labels={\"value\": \"Demanda (MWh)\", \"hour\": \"Hora del Día\"}\n",
    "            )\n",
    "\n",
    "\n",
    "            st.plotly_chart(fig2, use_container_width=True)\n",
    "\n",
    "        else:\n",
    "            st.markdown(\"Nada que ver... de momento\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70457de-035c-4eba-a965-bc1d501faa2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b55a5f-0be2-407f-a351-5651dd8ab537",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a633ac-ef3a-4fa0-9a28-56125917a1a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3acc5c2c-f14e-421f-9320-4de25612d058",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dd5c722-d590-4942-ba64-68d457457357",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f0940e-e943-473e-995d-caf517c39f15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a6066f-ba03-4200-82c7-7cdd6cb76adc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "541d19b7-ab8d-4609-b283-d78fb4d827b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dcc7aa-1066-410f-8d76-5a2d2ed78830",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c36cc4-5502-45db-9606-c86a67c21682",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc153e-d5d4-42a0-9049-afecd6bdea13",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5532487c-45ef-484b-88cb-356d2cba0cb4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca9d41e-ae36-465c-9731-07649e1460c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ac22e-0134-4e99-919f-f2609f2c54cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba19e134-a927-4721-af75-82c488b037a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0907ccc6-c987-4085-b26f-48fb5154aa66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd8ecd3-8eda-44cd-9fcf-7def45963538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e791144a-396a-4eab-9bd4-9b75becc4a5a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
