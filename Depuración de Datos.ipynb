{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9e7d78-27f6-416f-9acf-2eb4a1525283",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f8ba1e-98ca-4bc1-a092-49864d9062c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. FUNCION DE IMPUTACION ESTACIONAL\n",
    "\n",
    "def imputar_datos_estacionales(df):\n",
    "    \"\"\"\n",
    "    Imputa valores NaN en la columna 'value' basándose en la media por hora y día de la semana.\n",
    "    Si no hay NaNs, retorna el mismo DataFrame.\n",
    "    \"\"\"\n",
    "    if df['value'].isna().sum() == 0:\n",
    "        print(\" No se encontraron valores NaN en 'value'.\")\n",
    "        return df\n",
    "\n",
    "    print(\" Se encontraron valores NaN. Imputando...\")\n",
    "\n",
    "    df['weekday'] = df['datetime'].dt.dayofweek  # Lunes=0, Domingo=6\n",
    "    df['hour'] = df['datetime'].dt.hour\n",
    "\n",
    "    media_estacional = (\n",
    "        df.groupby(['weekday', 'hour'])['value']\n",
    "        .mean()\n",
    "        .reset_index()\n",
    "        .rename(columns={'value': 'media_estacional'})\n",
    "    )\n",
    "\n",
    "    df = df.merge(media_estacional, on=['weekday', 'hour'], how='left')\n",
    "    df['value'] = df['value'].fillna(df['media_estacional'])\n",
    "    df = df.drop(columns=['media_estacional'])\n",
    "\n",
    "    print(\" Imputación completada usando estacionalidad.\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8dc64d-4004-4eb4-be1b-a1b6ed8b41b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. FUNCION DE ESCALADO Y GUARDADO DEL SCALER\n",
    "\n",
    "def escalar_datos(df, save_path='scaler.pkl'):\n",
    "    \"\"\"\n",
    "    Escala la columna 'value' usando StandardScaler y guarda el objeto scaler como pickle.\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    df['value_scaled'] = scaler.fit_transform(df[['value']])\n",
    "\n",
    "    with open(save_path, 'wb') as f:\n",
    "        pickle.dump(scaler, f)\n",
    "\n",
    "    print(f\" Datos escalados y scaler guardado en '{save_path}'.\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66e7d790-9a97-48f1-95bd-22714ed5663c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. FUNCION PARA DIVIDIR EN TRAIN Y TEST\n",
    "\n",
    "def dividir_train_test(df, test_size=0.2):\n",
    "    \"\"\"\n",
    "    Divide los datos en entrenamiento y prueba respetando el orden temporal.\n",
    "    \"\"\"\n",
    "    df = df.sort_values('datetime')\n",
    "    n_total = len(df)\n",
    "    n_test = int(n_total * test_size)\n",
    "    n_train = n_total - n_test\n",
    "\n",
    "    df_train = df.iloc[:n_train].reset_index(drop=True)\n",
    "    df_test = df.iloc[n_train:].reset_index(drop=True)\n",
    "\n",
    "    print(f\" División completada: {len(df_train)} train / {len(df_test)} test\")\n",
    "    return df_train, df_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca358122-ce54-4647-b2d3-25651180673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos\n",
    "df = pd.read_csv(\"demanda.csv\", parse_dates=['datetime'])\n",
    "print(\"Datos cargados:\")\n",
    "print(df.head())\n",
    "\n",
    "# Imputación\n",
    "df = imputar_datos_estacionales(df)\n",
    "print(\"Después de imputación:\")\n",
    "print(df.head())\n",
    "\n",
    "# Escalado\n",
    "df = escalar_datos(df)\n",
    "print(\"Después de escalado:\")\n",
    "print(df[['datetime', 'value', 'value_scaled']].head())\n",
    "\n",
    "# División train/test\n",
    "df_train, df_test = dividir_train_test(df)\n",
    "print(f\" Train shape: {df_train.shape}\")\n",
    "print(f\" Test shape: {df_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25999d2-1535-4a9b-815a-052057025f11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a40fb4-6075-4765-8787-bcbb72f651e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e3031b6-18ae-4ded-ba6e-d5871ea239b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95c7039-b0b1-4cb2-9bc9-c90631b58c71",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
