{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22883233-ba5c-411c-82e4-2d1c33bdebcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import streamlit as st\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from supabase import create_client, Client\n",
    "import schedule\n",
    "import threading\n",
    "import time as tiempo\n",
    "import uuid\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import json\n",
    "import folium\n",
    "from streamlit_folium import st_folium\n",
    "from folium.features import GeoJsonTooltip\n",
    "import branca.colormap as cm\n",
    "from branca.element import Template, MacroElement\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import onnxruntime as ort\n",
    "import joblib\n",
    "from prophet.plot import plot_plotly, plot_components_plotly\n",
    "\n",
    "\n",
    "st.set_page_config(page_title=\"Red Eléctrica\", layout=\"centered\")\n",
    "\n",
    "# Constantes de configuración de la API REE\n",
    "BASE_URL = \"https://apidatos.ree.es/es/datos/\"\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"content-type\": \"application/json\"\n",
    "}\n",
    "\n",
    "ENDPOINTS = {\n",
    "    \"demanda\": (\"demanda/evolucion\", \"hour\"),\n",
    "    \"balance\": (\"balance/balance-electrico\", \"day\"),\n",
    "    \"generacion\": (\"generacion/evolucion-renovable-no-renovable\", \"day\"),\n",
    "    \"intercambios\": (\"intercambios/todas-fronteras-programados\", \"day\"),\n",
    "    \"intercambios_baleares\": (\"intercambios/enlace-baleares\", \"day\"),\n",
    "}\n",
    "\n",
    "# Cargar las variables de entorno desde el archivo .env\n",
    "load_dotenv()\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "\n",
    "# ------------------------------ UTILIDADES ------------------------------\n",
    "\n",
    "# Función para consultar un endpoint, según los parámetros dados, de la API de REE\n",
    "def get_data(endpoint_name, endpoint_info, params):\n",
    "    path, time_trunc = endpoint_info\n",
    "    params[\"time_trunc\"] = time_trunc\n",
    "    url = BASE_URL + path\n",
    "\n",
    "    try:\n",
    "        response = requests.get(url, headers=HEADERS, params=params)\n",
    "        # Si la búsqueda no fue bien, se devuelve una lista vacía\n",
    "        if response.status_code != 200:\n",
    "            return []\n",
    "        response_data = response.json()\n",
    "    except Exception:\n",
    "        return []\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # Verificamos si el item tiene \"content\" y asumimos que es una estructura compleja\n",
    "    for item in response_data.get(\"included\", []):\n",
    "        attrs = item.get(\"attributes\", {})\n",
    "        category = attrs.get(\"title\")\n",
    "\n",
    "        if \"content\" in attrs:\n",
    "            for sub in attrs[\"content\"]:\n",
    "                sub_attrs = sub.get(\"attributes\", {})\n",
    "                sub_cat = sub_attrs.get(\"title\")\n",
    "                for entry in sub_attrs.get(\"values\", []):\n",
    "                    entry[\"primary_category\"] = category\n",
    "                    entry[\"sub_category\"] = sub_cat\n",
    "                    data.append(entry)\n",
    "        else:\n",
    "            # Procesamos las estructuras más simples (demanda, generacion, intercambios_baleares), asumiendo que no hay subcategorías\n",
    "            for entry in attrs.get(\"values\", []):\n",
    "                entry[\"primary_category\"] = category\n",
    "                entry[\"sub_category\"] = None\n",
    "                data.append(entry)\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "# Función para insertar cada DataFrame en Supabase\n",
    "def insertar_en_supabase(nombre_tabla, df):\n",
    "    df = df.copy()\n",
    "\n",
    "    # Generamos IDs únicos\n",
    "    df[\"record_id\"] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "    # Convertimos fechas a string ISO\n",
    "    for col in [\"datetime\", \"extraction_timestamp\"]:\n",
    "        if col in df.columns:\n",
    "            df[col] = df[col].astype(str)\n",
    "\n",
    "    # Reemplazamos NaN por None\n",
    "    # df = df.where(pd.notnull(df), None)\n",
    "\n",
    "    # Convertir a lista de diccionarios e insertar\n",
    "    data = df.to_dict(orient=\"records\")\n",
    "\n",
    "    try:\n",
    "        supabase.table(nombre_tabla).insert(data).execute()\n",
    "        print(f\"✅ Insertados en '{nombre_tabla}': {len(data)} filas\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error al insertar en '{nombre_tabla}': {e}\")\n",
    "\n",
    "\n",
    "# ------------------------------ FUNCIONES DE DESCARGA ------------------------------\n",
    "# Función de extracción de datos de los últimos x años, devuelve DataFrame. Ejecutar una vez al inicio para poblar la base de datos.\n",
    "def get_data_for_last_x_years(num_years=3):\n",
    "    all_dfs = []\n",
    "    current_date = datetime.now()\n",
    "    # Calculamos el año de inicio a partir del año actual\n",
    "    start_year_limit = current_date.year - num_years\n",
    "\n",
    "    # Iteramos sobre cada año y mes\n",
    "    for year in range(start_year_limit, current_date.year + 1):\n",
    "        for month in range(1, 13):\n",
    "            # Si el mes es mayor al mes actual y el año es el actual, lo saltamos\n",
    "            month_start = datetime(year, month, 1)\n",
    "            if month_start > current_date:\n",
    "                continue\n",
    "            # Calculamos el final del mes, asegurándonos de no exceder la fecha actual\n",
    "            month_end = (month_start + timedelta(days=32)).replace(day=1) - timedelta(minutes=1)\n",
    "            end_date_for_request = min(month_end, current_date)\n",
    "\n",
    "            monthly_data = []  # para acumular todos los dfs del mes\n",
    "\n",
    "            # Iteramos sobre cada endpoint y sacamos los datos\n",
    "            for name, (path, granularity) in ENDPOINTS.items():\n",
    "                params = {\n",
    "                    \"start_date\": month_start.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "                    \"end_date\": end_date_for_request.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "                    \"geo_trunc\": \"electric_system\",\n",
    "                    \"geo_limit\": \"peninsular\",\n",
    "                    \"geo_ids\": \"8741\"\n",
    "                }\n",
    "\n",
    "                data = get_data(name, (path, granularity), params)\n",
    "\n",
    "                if data:\n",
    "                    df = pd.DataFrame(data)\n",
    "                    # Lidiamos con problemas de zona horaria en la columna \"datetime\"\n",
    "                    try:\n",
    "                        df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "                    except Exception:\n",
    "                        continue\n",
    "\n",
    "                    # Obtenemos nuevas columnas y las reordenamos\n",
    "                    df['year'] = df['datetime'].dt.year\n",
    "                    df['month'] = df['datetime'].dt.month\n",
    "                    df['day'] = df['datetime'].dt.day\n",
    "                    df['hour'] = df['datetime'].dt.hour\n",
    "                    df['extraction_timestamp'] = datetime.utcnow()\n",
    "                    df['endpoint'] = name\n",
    "                    df['record_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "                    df = df[['record_id', 'value', 'percentage', 'datetime',\n",
    "                             'primary_category', 'sub_category', 'year', 'month',\n",
    "                             'day', 'hour', 'endpoint', 'extraction_timestamp']]\n",
    "\n",
    "                    monthly_data.append(df)\n",
    "                    tiempo.sleep(1)\n",
    "\n",
    "            # Generamos los dataframes individuales\n",
    "            if monthly_data:\n",
    "                df_nuevo = pd.concat(monthly_data, ignore_index=True)\n",
    "                all_dfs.append(df_nuevo)\n",
    "\n",
    "                tablas_dfs = {\n",
    "                    \"demanda\": df_nuevo[df_nuevo[\"endpoint\"] == \"demanda\"].drop(columns=[\"endpoint\", \"sub_category\"],\n",
    "                                                                                errors='ignore'),\n",
    "                    \"balance\": df_nuevo[df_nuevo[\"endpoint\"] == \"balance\"].drop(columns=[\"endpoint\"], errors='ignore'),\n",
    "                    \"generacion\": df_nuevo[df_nuevo[\"endpoint\"] == \"generacion\"].drop(\n",
    "                        columns=[\"endpoint\", \"sub_category\"], errors='ignore'),\n",
    "                    \"intercambios\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios\"].drop(columns=[\"endpoint\"],\n",
    "                                                                                          errors='ignore'),\n",
    "                    \"intercambios_baleares\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios_baleares\"].drop(\n",
    "                        columns=[\"endpoint\", \"sub_category\"], errors='ignore'),\n",
    "                }\n",
    "\n",
    "                for tabla, df_tabla in tablas_dfs.items():\n",
    "                    if not df_tabla.empty:\n",
    "                        insertar_en_supabase(tabla, df_tabla)\n",
    "\n",
    "    return pd.concat(all_dfs, ignore_index=True) if all_dfs else pd.DataFrame()\n",
    "\n",
    "\n",
    "# Función para actualizar los datos desde la API cada 24 horas\n",
    "def actualizar_datos_desde_api():\n",
    "    print(f\"[{datetime.now()}] ⏳ Ejecutando extracción desde API...\")\n",
    "    current_date = datetime.now()\n",
    "    start_date = current_date - timedelta(days=1)\n",
    "\n",
    "    all_dfs = []\n",
    "\n",
    "    for name, (path, granularity) in ENDPOINTS.items():\n",
    "        params = {\n",
    "            \"start_date\": start_date.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "            \"end_date\": current_date.strftime(\"%Y-%m-%dT%H:%M\"),\n",
    "            \"geo_trunc\": \"electric_system\",\n",
    "            \"geo_limit\": \"peninsular\",\n",
    "            \"geo_ids\": \"8741\"\n",
    "        }\n",
    "\n",
    "        datos = get_data(name, (path, granularity), params)\n",
    "\n",
    "        if datos:\n",
    "            df = pd.DataFrame(datos)\n",
    "            try:\n",
    "                df['datetime'] = pd.to_datetime(df['datetime'], utc=True)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            df['year'] = df['datetime'].dt.year\n",
    "            df['month'] = df['datetime'].dt.month\n",
    "            df['day'] = df['datetime'].dt.day\n",
    "            df['hour'] = df['datetime'].dt.hour\n",
    "            df['extraction_timestamp'] = datetime.utcnow()\n",
    "            df['endpoint'] = name\n",
    "            df['record_id'] = [str(uuid.uuid4()) for _ in range(len(df))]\n",
    "\n",
    "            df = df[['record_id', 'value', 'percentage', 'datetime',\n",
    "                     'primary_category', 'sub_category', 'year', 'month',\n",
    "                     'day', 'hour', 'endpoint', 'extraction_timestamp']]\n",
    "\n",
    "            all_dfs.append(df)\n",
    "            tiempo.sleep(1)\n",
    "        else:\n",
    "            print(f\"⚠️ No se obtuvieron datos de '{name}'\")\n",
    "\n",
    "    if all_dfs:\n",
    "        df_nuevo = pd.concat(all_dfs, ignore_index=True)\n",
    "\n",
    "        tablas_dfs = {\n",
    "            \"demanda\": df_nuevo[df_nuevo[\"endpoint\"] == \"demanda\"].drop(columns=[\"endpoint\", \"sub_category\"]),\n",
    "            \"balance\": df_nuevo[df_nuevo[\"endpoint\"] == \"balance\"].drop(columns=[\"endpoint\"]),\n",
    "            \"generacion\": df_nuevo[df_nuevo[\"endpoint\"] == \"generacion\"].drop(columns=[\"endpoint\", \"sub_category\"]),\n",
    "            \"intercambios\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios\"].drop(columns=[\"endpoint\"]),\n",
    "            \"intercambios_baleares\": df_nuevo[df_nuevo[\"endpoint\"] == \"intercambios_baleares\"].drop(\n",
    "                columns=[\"endpoint\", \"sub_category\"]),\n",
    "        }\n",
    "\n",
    "        for tabla, df in tablas_dfs.items():\n",
    "            if not df.empty:\n",
    "                insertar_en_supabase(tabla, df)\n",
    "\n",
    "\n",
    "# Programador para actualizar datos desde la API cada 24 horas\n",
    "def iniciar_programador_api():\n",
    "    schedule.every(24).hours.do(actualizar_datos_desde_api)\n",
    "    while True:\n",
    "        schedule.run_pending()\n",
    "        tiempo.sleep(60)\n",
    "\n",
    "\n",
    "# threading.Thread(target=iniciar_programador_api, daemon=True).start()\n",
    "\n",
    "# ------------------------------ CONSULTA SUPABASE ------------------------------\n",
    "\n",
    "def get_data_from_supabase(table_name, start_date, end_date, page_size=1000):\n",
    "    end_date += timedelta(days=1)\n",
    "    start_iso = start_date.isoformat()\n",
    "    end_iso = end_date.isoformat()\n",
    "\n",
    "    all_data = []\n",
    "    offset = 0\n",
    "    while True:\n",
    "        response = (\n",
    "            supabase.table(table_name)\n",
    "            .select(\"*\")\n",
    "            .gte(\"datetime\", start_iso)\n",
    "            .lte(\"datetime\", end_iso)\n",
    "            .range(offset, offset + page_size - 1)\n",
    "            .execute()\n",
    "        )\n",
    "        data = response.data\n",
    "        if not data:\n",
    "            break\n",
    "        all_data.extend(data)\n",
    "        offset += page_size\n",
    "        if len(data) < page_size:\n",
    "            break\n",
    "\n",
    "    if not all_data:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.DataFrame(all_data)\n",
    "    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n",
    "    return df\n",
    "\n",
    "# ------------------------------ INTERFAZ ------------------------------\n",
    "\n",
    "\n",
    "def mostrar_estructura_base_datos():\n",
    "    st.subheader(\"Arquitectura de la Base de Datos\")\n",
    "\n",
    "    st.markdown(\"\"\"\n",
    "    Este proyecto utiliza **Supabase** como base de datos principal, organizada en 5 tablas que reflejan distintos aspectos de los datos de Red Eléctrica Española (REE):\n",
    "\n",
    "    ###  Tablas y Columnas Clave\n",
    "\n",
    "    #### 1. `demanda`\n",
    "    - **Descripción**: Demanda eléctrica horaria.\n",
    "    - **Columnas**: `datetime`, `value`, `primary_category`, `year`, `month`, `day`, `hour`.\n",
    "\n",
    "    #### 2. `balance`\n",
    "    - **Descripción**: Balance eléctrico diario por categorías.\n",
    "    - **Columnas**: `datetime`, `value`, `primary_category`.\n",
    "\n",
    "    #### 3. `generacion`\n",
    "    - **Descripción**: Energía generada por tipo (solar, eólica, etc.).\n",
    "    - **Columnas**: `datetime`, `value`, `primary_category`.\n",
    "\n",
    "    #### 4. `intercambios`\n",
    "    - **Descripción**: Intercambios internacionales (Francia, Marruecos...).\n",
    "    - **Columnas**: `datetime`, `value`, `primary_category`.\n",
    "\n",
    "    #### 5. `intercambios_baleares`\n",
    "    - **Descripción**: Flujo energético entre península y Baleares.\n",
    "    - **Columnas**: `datetime`, `value`, `primary_category`.\n",
    "\n",
    "    ###  Columnas Comunes\n",
    "    - `record_id`: UUID único.\n",
    "    - `extraction_timestamp`: Fecha/hora en que se extrajo el dato.\n",
    "    - `endpoint`: Origen de los datos.\n",
    "    \"\"\")\n",
    "\n",
    "    st.markdown(\"Además, se puede consultar directamente cada tabla desde la pestaña *Consulta de Datos*.\")\n",
    "def main():\n",
    "    st.title(\"Análisis de la Red Eléctrica Española\")\n",
    "    \n",
    "#CAMBIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "    tab1, tab2, tab3, tab4, tab5, tab6, tab7, tab8 = st.tabs([\n",
    "    \"Descripción\", \"Consulta de datos\", \"Visualización\", \"Predicciones\",\n",
    "    \"Prophet\", \"Extras\", \"Base de Datos\", \"About Us\"\n",
    "])\n",
    "\n",
    "\n",
    "    with tab2:  # Mueve el contexto de la tab2 aquí para que `modo` se defina antes de usarse en session_state\n",
    "        st.subheader(\"Consulta de datos\")\n",
    "\n",
    "        modo = st.radio(\"Tipo de consulta:\", [\"Últimos días\", \"Año específico\", \"Histórico\"], horizontal=True,\n",
    "                        key=\"query_mode_radio\")\n",
    "        st.session_state[\"modo_seleccionado\"] = modo  # Guardar el modo en session_state\n",
    "\n",
    "        tabla = st.selectbox(\"Selecciona la tabla:\", list(ENDPOINTS.keys()), key=\"query_table_select\")\n",
    "\n",
    "        df = pd.DataFrame()  # Inicializamos el DataFrame para evitar errores\n",
    "\n",
    "        if modo == \"Últimos días\":\n",
    "            dias = st.selectbox(\"¿Cuántos días atrás?\", [7, 14, 30], key=\"query_days_select\")\n",
    "            end_date_query = datetime.now(timezone.utc)\n",
    "            start_date_query = end_date_query - timedelta(days=dias)\n",
    "            st.session_state[\"selected_year_for_viz\"] = None  # Reset year if not in \"Año específico\" mode\n",
    "        elif modo == \"Año específico\":\n",
    "            current_year = datetime.now().year\n",
    "            # Se usa `key` para que el selectbox mantenga su estado\n",
    "            año = st.selectbox(\"Selecciona el año:\", [current_year - i for i in range(3)], index=0,\n",
    "                               key=\"query_year_select\")\n",
    "            st.session_state[\"selected_year_for_viz\"] = año  # Store the selected year\n",
    "            start_date_query = datetime(año, 1, 1, tzinfo=timezone.utc)\n",
    "            end_date_query = datetime(año, 12, 31, 23, 59, 59, 999999, tzinfo=timezone.utc)\n",
    "        elif modo == \"Histórico\":\n",
    "            # Si quieres cargar datos históricos de muchos años, ten cuidado con el rendimiento\n",
    "            start_date_query = datetime(2022, 1, 1, 0, 0, 0, tzinfo=timezone.utc)  # Ejemplo de fecha inicial\n",
    "            end_date_query = datetime.now(timezone.utc)\n",
    "            st.session_state[\"selected_year_for_viz\"] = None  # Reset year if not in \"Año específico\" mode\n",
    "\n",
    "        with st.spinner(\"Consultando Supabase...\"):\n",
    "            st.session_state[\"tabla_seleccionada_en_tab2\"] = tabla\n",
    "            df = get_data_from_supabase(tabla, start_date_query, end_date_query)\n",
    "\n",
    "        # Mostrar resultados después de la consulta de cualquier modo\n",
    "        if not df.empty:\n",
    "            st.session_state[\"ree_data\"] = df\n",
    "            st.session_state[\n",
    "                \"tabla\"] = tabla  # Esto es redundante si usas tabla_seleccionada_en_tab2, pero lo mantengo por seguridad\n",
    "            st.write(f\"Datos recuperados: {len(df)} filas\")\n",
    "            st.write(\"Último dato:\", df['datetime'].max())\n",
    "            st.success(\"Datos cargados correctamente desde Supabase.\")\n",
    "        else:\n",
    "            st.warning(\"No se encontraron datos para ese período.\")\n",
    "\n",
    "    with tab1:  # Reordeno esto para que la tab2 se cargue primero y defina el estado\n",
    "        st.subheader(\"¿Qué es esta app?\")\n",
    "        st.markdown(\"\"\"\n",
    "        Este proyecto explora los datos públicos de la **Red Eléctrica de España (REE)** a través de su API.\n",
    "        Se analizan aspectos como:\n",
    "\n",
    "        - La **demanda eléctrica** por hora.\n",
    "        - El **balance eléctrico** por día.\n",
    "        - La **generación** por mes.\n",
    "        - Los **intercambios programados** con otros países.\n",
    "\n",
    "        Estos datos permiten visualizar la evolución energética de España y generar análisis útiles para planificación y sostenibilidad.\n",
    "        \"\"\")\n",
    "\n",
    "    with tab3:\n",
    "        st.subheader(\"Visualización\")\n",
    "        if \"ree_data\" in st.session_state and not st.session_state[\"ree_data\"].empty:\n",
    "            # Recuperamos el DataFrame principal de la sesión para el primer gráfico\n",
    "            df = st.session_state[\"ree_data\"]\n",
    "            tabla = st.session_state[\"tabla_seleccionada_en_tab2\"]  # Usamos la tabla seleccionada en tab2\n",
    "            modo_actual = st.session_state.get(\"modo_seleccionado\", \"Últimos días\")  # Obtener el modo\n",
    "\n",
    "            if tabla == \"demanda\":\n",
    "                fig = px.area(df, x=\"datetime\", y=\"value\", title=\"Demanda Eléctrica\", labels={\"value\": \"MW\"})\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "                # --- Nuevo gráfico: Histograma de demanda con outliers para año específico ---\n",
    "                if modo_actual == \"Año específico\":\n",
    "                    año_seleccionado = st.session_state.get(\"selected_year_for_viz\")\n",
    "                    if año_seleccionado is None:\n",
    "                        st.warning(\n",
    "                            \"Por favor, selecciona un año en la pestaña 'Consulta de datos' para ver el histograma de demanda.\")\n",
    "                    else:\n",
    "                        st.subheader(f\"Distribución de Demanda y Valores Atípicos para el año {año_seleccionado}\")\n",
    "\n",
    "                        # Filtra el DataFrame para el año seleccionado (df ya debe estar filtrado por el año, pero esto es por seguridad)\n",
    "                        df_año = df[df['year'] == año_seleccionado].copy()\n",
    "\n",
    "                        if not df_año.empty:\n",
    "                            # Calcular Q1, Q3 y el IQR para la columna 'value' (demanda)\n",
    "                            Q1 = df_año['value'].quantile(0.25)\n",
    "                            Q3 = df_año['value'].quantile(0.75)\n",
    "                            IQR = Q3 - Q1\n",
    "\n",
    "                            # Calcular los límites de Tukey's Fence\n",
    "                            lower_bound = Q1 - 1.5 * IQR\n",
    "                            upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                            # Identificar valores atípicos\n",
    "                            df_año['is_outlier'] = 'Normal'\n",
    "                            df_año.loc[df_año['value'] < lower_bound, 'is_outlier'] = 'Atípico (bajo)'\n",
    "                            df_año.loc[df_año['value'] > upper_bound, 'is_outlier'] = 'Atípico (alto)'\n",
    "\n",
    "                            # Crear el histograma\n",
    "                            fig_hist_outliers = px.histogram(\n",
    "                                df_año,\n",
    "                                x=\"value\",\n",
    "                                color=\"is_outlier\",\n",
    "                                title=f\"Distribución Horaria de Demanda para {año_seleccionado}\",\n",
    "                                labels={\"value\": \"Demanda (MW)\", \"is_outlier\": \"Tipo de Valor\"},\n",
    "                                category_orders={\"is_outlier\": [\"Atípico (bajo)\", \"Normal\", \"Atípico (alto)\"]},\n",
    "                                color_discrete_map={'Normal': 'skyblue', 'Atípico (bajo)': 'orange',\n",
    "                                                    'Atípico (alto)': 'red'},\n",
    "                                nbins=50  # Ajusta el número de bins según la granularidad deseada\n",
    "                            )\n",
    "                            fig_hist_outliers.update_layout(bargap=0.1)  # Espacio entre barras\n",
    "                            st.plotly_chart(fig_hist_outliers, use_container_width=True)\n",
    "\n",
    "                            # Mostrar información sobre outliers\n",
    "                            num_outliers_low = (df_año['is_outlier'] == 'Atípico (bajo)').sum()\n",
    "                            num_outliers_high = (df_año['is_outlier'] == 'Atípico (alto)').sum()\n",
    "\n",
    "                            if num_outliers_low > 0 or num_outliers_high > 0:\n",
    "                                st.warning(\n",
    "                                    f\"Se han identificado {num_outliers_low} valores atípicos por debajo y {num_outliers_high} por encima (método IQR).\")\n",
    "                                st.info(f\"Rango normal de demanda (IQR): {lower_bound:.2f} MW - {upper_bound:.2f} MW\")\n",
    "                            else:\n",
    "                                st.info(\n",
    "                                    \"No se han identificado valores atípicos de demanda significativos (método IQR).\")\n",
    "                        else:\n",
    "                            st.warning(\n",
    "                                f\"No hay datos de demanda para el año {año_seleccionado} para generar el histograma.\")\n",
    "\n",
    "                # --- Condición para mostrar la comparativa de años (mantenida de antes) ---\n",
    "                if modo_actual == \"Histórico\":\n",
    "                    st.subheader(\"Comparativa de Demanda entre años\")\n",
    "\n",
    "                    # Definir los dos años específicos para la comparación: el año pasado y el anterior\n",
    "                    current_year = datetime.now().year\n",
    "\n",
    "                    # Los años que queremos comparar: el año anterior al actual y el año anterior a ese\n",
    "                    target_years_for_comparison = [current_year - 2, current_year - 1]\n",
    "\n",
    "                    # Obtener todos los años disponibles en el DataFrame del modo histórico\n",
    "                    all_available_years_in_df = sorted(list(df['year'].unique()))\n",
    "\n",
    "                    # Filtrar solo los años que queremos comparar y que realmente están disponibles en el df\n",
    "                    years_for_comparison = [\n",
    "                        year for year in target_years_for_comparison\n",
    "                        if year in all_available_years_in_df\n",
    "                    ]\n",
    "\n",
    "                    if len(years_for_comparison) == 2:\n",
    "                        # Asegurarse de que están en el orden deseado (ej. [2023, 2024])\n",
    "                        years_for_comparison.sort()\n",
    "                    elif len(years_for_comparison) == 1:\n",
    "                        st.info(\n",
    "                            f\"Solo se encontró un año de los deseados ({years_for_comparison[0]}) en modo histórico para la comparación. Se necesitan ambos años ({target_years_for_comparison[0]} y {target_years_for_comparison[1]}).\")\n",
    "                        years_for_comparison = []  # Vaciar para no intentar graficar\n",
    "                    else:  # len(years_for_comparison) == 0\n",
    "                        st.info(\n",
    "                            f\"No se encontraron datos para los años {target_years_for_comparison[0]} y {target_years_for_comparison[1]} en modo histórico para la comparación.\")\n",
    "                        years_for_comparison = []  # Vaciar para no intentar graficar\n",
    "\n",
    "                    if years_for_comparison:  # Solo procede si tenemos al menos dos años para comparar\n",
    "                        df_comparison_demanda = df.copy()  # Usamos el df ya cargado en modo histórico\n",
    "\n",
    "                        # Nos aseguramos de que solo tengamos los años que queremos comparar\n",
    "                        df_filtered_comparison = df_comparison_demanda[\n",
    "                            df_comparison_demanda['year'].isin(years_for_comparison)].copy()\n",
    "\n",
    "                        # Convertimos la columna 'datetime' a una fecha sin el año, para comparar día a día\n",
    "                        # Esta 'sort_key' se usa para el gráfico horario\n",
    "                        df_filtered_comparison['sort_key'] = df_filtered_comparison['datetime'].apply(\n",
    "                            lambda dt: dt.replace(year=2000)  # Usar un año base para ordenar correctamente\n",
    "                        )\n",
    "                        df_filtered_comparison = df_filtered_comparison.sort_values('sort_key')\n",
    "\n",
    "                        # --- Gráfico de Demanda Horaria General Comparativa ---\n",
    "                        fig_comp_hourly = px.line(\n",
    "                            df_filtered_comparison,\n",
    "                            x=\"sort_key\",  # Usamos la 'sort_key' que es datetime\n",
    "                            y=\"value\",\n",
    "                            color=\"year\",\n",
    "                            title=\"Demanda Horaria - Comparativa\",\n",
    "                            labels={\"sort_key\": \"Mes y Día\", \"value\": \"Demanda (MW)\", \"year\": \"Año\"},\n",
    "                            hover_data={\"year\": True, \"datetime\": \"|%Y-%m-%d %H:%M\"}\n",
    "                        )\n",
    "                        fig_comp_hourly.update_xaxes(tickformat=\"%b %d\")  # Formato para mostrar Mes y Día en el eje X\n",
    "                        st.plotly_chart(fig_comp_hourly, use_container_width=True)\n",
    "\n",
    "                        # --- Gráficos de Comparación de Métricas Diarias (Media, Mediana, Mínima, Máxima) ---\n",
    "                        # Agrupar por 'year' y 'month-day' para obtener las métricas diarias para cada año\n",
    "                        metrics_comp = df_filtered_comparison.groupby(\n",
    "                            ['year', df_filtered_comparison['datetime'].dt.strftime('%m-%d')])['value'].agg(\n",
    "                            ['mean', 'median', 'min', 'max']).reset_index()\n",
    "                        metrics_comp.columns = ['year', 'month_day', 'mean', 'median', 'min', 'max']\n",
    "\n",
    "                        # La corrección para el ValueError: day is out of range for month está aquí\n",
    "                        metrics_comp['sort_key'] = pd.to_datetime('2000-' + metrics_comp['month_day'],\n",
    "                                                                  format='%Y-%m-%d')\n",
    "                        metrics_comp = metrics_comp.sort_values('sort_key')\n",
    "\n",
    "                        metric_names = {\n",
    "                            'mean': 'Media diaria de demanda',\n",
    "                            'median': 'Mediana diaria de demanda',\n",
    "                            'min': 'Mínima diaria de demanda',\n",
    "                            'max': 'Máxima diaria de demanda',\n",
    "                        }\n",
    "\n",
    "                        for metric in ['mean', 'median', 'min', 'max']:\n",
    "                            fig = px.line(\n",
    "                                metrics_comp,\n",
    "                                x=\"sort_key\",  # <--- CAMBIO CLAVE: Usar 'sort_key' (tipo datetime) para el eje X\n",
    "                                y=metric,\n",
    "                                color=\"year\",\n",
    "                                title=metric_names[metric],\n",
    "                                labels={\"sort_key\": \"Fecha (Mes-Día)\", metric: \"Demanda (MW)\", \"year\": \"Año\"},\n",
    "                                # <--- CAMBIO EN ETIQUETA\n",
    "                            )\n",
    "                            fig.update_xaxes(tickformat=\"%b %d\")  # Formato para mostrar solo Mes y Día\n",
    "                            # Si las líneas siguen entrecortadas, considera añadir `connectgaps=True`\n",
    "                            # fig.update_traces(connectgaps=True)\n",
    "                            st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        st.warning(f\"No hay suficientes datos de Demanda disponibles para la comparación.\")\n",
    "\n",
    "                    # --- Gráfico de Identificación de años outliers (mantenida de antes) ---\n",
    "                    st.subheader(\"Identificación de Años Outliers (Demanda Anual Total)\")\n",
    "\n",
    "                    st.markdown(\n",
    "                        \"**Este gráfico muestra los años identificados como outliers en la demanda total anual.**\\n\\n\"\n",
    "                        \"En este caso, solo se detecta como outlier el año **2025**, lo cual es esperable ya que todavía no ha finalizado \"\n",
    "                        \"y su demanda acumulada es significativamente menor.\\n\\n\"\n",
    "                        \"Los años **2022, 2023 y 2024** presentan una demanda anual muy similar, en torno a los **700 MW**, por lo que \"\n",
    "                        \"no se consideran outliers según el criterio del rango intercuartílico (IQR).\"\n",
    "                    )\n",
    "\n",
    "                    # Asegurarse de que el df tiene la columna 'year'\n",
    "                    if 'year' not in df.columns:\n",
    "                        df['year'] = df['datetime'].dt.year\n",
    "\n",
    "                    # Agrupar por año para obtener la demanda total anual\n",
    "                    df_annual_summary = df.groupby('year')['value'].sum().reset_index()\n",
    "                    df_annual_summary.rename(columns={'value': 'total_demand_MW'}, inplace=True)\n",
    "\n",
    "                    if not df_annual_summary.empty and len(df_annual_summary) > 1:\n",
    "                        # Calcular Q1, Q3 y el IQR\n",
    "                        Q1 = df_annual_summary['total_demand_MW'].quantile(0.25)\n",
    "                        Q3 = df_annual_summary['total_demand_MW'].quantile(0.75)\n",
    "                        IQR = Q3 - Q1\n",
    "\n",
    "                        # Calcular los límites para los outliers\n",
    "                        lower_bound = Q1 - 1.5 * IQR\n",
    "                        upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "                        # Identificar los años que son outliers\n",
    "                        df_annual_summary['is_outlier'] = (\n",
    "                                (df_annual_summary['total_demand_MW'] < lower_bound) |\n",
    "                                (df_annual_summary['total_demand_MW'] > upper_bound)\n",
    "                        )\n",
    "\n",
    "                        # Crear el gráfico de barras\n",
    "                        fig_outliers = px.bar(\n",
    "                            df_annual_summary,\n",
    "                            x='year',\n",
    "                            y='total_demand_MW',\n",
    "                            color='is_outlier',  # Colorear las barras si son outliers\n",
    "                            title='Demanda Total Anual y Años Outlier',\n",
    "                            labels={'total_demand_MW': 'Demanda Total Anual (MW)', 'year': 'Año',\n",
    "                                    'is_outlier': 'Es Outlier'},\n",
    "                            color_discrete_map={False: 'skyblue', True: 'red'}  # Definir colores\n",
    "                        )\n",
    "\n",
    "                        st.plotly_chart(fig_outliers, use_container_width=True)\n",
    "\n",
    "                        # Mostrar los años identificados como outliers\n",
    "                        outlier_years = df_annual_summary[df_annual_summary['is_outlier']]['year'].tolist()\n",
    "                        if outlier_years:\n",
    "                            st.warning(\n",
    "                                f\"Se han identificado los siguientes años como outliers: {', '.join(map(str, outlier_years))}\")\n",
    "                        else:\n",
    "                            st.info(\"No se han identificado años outliers significativos (según el método IQR).\")\n",
    "                    elif not df_annual_summary.empty and len(df_annual_summary) <= 1:\n",
    "                        st.info(\"Se necesitan al menos 2 años de datos para calcular outliers de demanda anual.\")\n",
    "                    else:\n",
    "                        st.warning(\"No hay datos anuales disponibles para calcular outliers.\")\n",
    "                # El siguiente 'else' se aplica si modo_actual NO es \"Histórico\"\n",
    "                elif modo_actual != \"Histórico\" and modo_actual != \"Año específico\":\n",
    "                    st.info(\n",
    "                        \"Selecciona el modo 'Histórico' para ver la comparativa de años y la identificación de outliers anuales, o 'Año específico' para el histograma de demanda con outliers.\")\n",
    "\n",
    "            elif tabla == \"balance\":\n",
    "\n",
    "                df_balance = df.groupby([df[\"datetime\"].dt.date, \"primary_category\"])[\"value\"].sum().reset_index()\n",
    "                df_balance.rename(columns={\"datetime\": \"date\"}, inplace=True)\n",
    "                df_balance = df_balance.sort_values(\"date\")\n",
    "                fig = px.line(\n",
    "                    df_balance,\n",
    "                    x=\"date\",\n",
    "                    y=\"value\",\n",
    "                    color=\"primary_category\",\n",
    "                    title=\"Balance Eléctrico Diario\"\n",
    "                )\n",
    "\n",
    "                fig.update_layout(\n",
    "                    xaxis_title=\"Fecha\",\n",
    "                    yaxis_title=\"MWh\",\n",
    "                    legend_title=\"Categoría\",\n",
    "                    template=\"plotly_white\"\n",
    "                )\n",
    "\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "                st.markdown(\n",
    "                    \"**Balance eléctrico diario por categoría**\\n\\n\"\n",
    "                    \"Este gráfico representa el balance energético entre las distintas fuentes y usos diarios. Cada barra agrupa los componentes \"\n",
    "                    \"principales del sistema: generación, consumo, pérdidas y exportaciones.\\n\\n\"\n",
    "                    \"Es útil para entender si hay superávit, déficit o equilibrio en la red cada día, y cómo se distribuye el uso de energía entre sectores.\"\n",
    "                )\n",
    "\n",
    "            elif tabla == \"generacion\":\n",
    "                df['date'] = df['datetime'].dt.date  # Para reducir a nivel diario (si no lo tienes)\n",
    "\n",
    "                df_grouped = df.groupby(['date', 'primary_category'])['value'].sum().reset_index()\n",
    "\n",
    "                fig = px.line(\n",
    "                    df_grouped,\n",
    "                    x=\"date\",\n",
    "                    y=\"value\",\n",
    "                    color=\"primary_category\",\n",
    "                    title=\"Generación diaria agregada por tipo\"\n",
    "                )\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "                st.markdown(\n",
    "                    \"**Generación diaria agregada por tipo**\\n\\n\"\n",
    "                    \"Se visualiza la evolución de la generación eléctrica por fuente: renovables (eólica, solar, hidroeléctrica) y no renovables \"\n",
    "                    \"(gas, nuclear, etc.).\\n\\n\"\n",
    "                    \"Esta gráfica permite observar patrones como aumentos de producción renovable en días soleados o ventosos, así como la estabilidad \"\n",
    "                    \"de tecnologías de base como la nuclear. Es clave para analizar la transición energética.\"\n",
    "                )\n",
    "\n",
    "\n",
    "            elif tabla == \"intercambios\":\n",
    "                st.subheader(\"Mapa Coroplético de Intercambios Eléctricos\")\n",
    "\n",
    "                st.markdown(\n",
    "                    \"**Intercambios eléctricos internacionales**\\n\\n\"\n",
    "                    \"Este mapa muestra el **saldo neto de energía** (exportaciones menos importaciones) entre España y los países vecinos: \"\n",
    "                    \"**Francia, Portugal, Marruecos y Andorra**.\\n\\n\"\n",
    "                    \"Los valores positivos indican que **España exporta más energía de la que importa**, mientras que los negativos reflejan lo contrario.\\n\\n\"\n",
    "                    \"Este análisis es clave para comprender el papel de España como nodo energético regional, identificar dependencias o excedentes, \"\n",
    "                    \"y analizar cómo varían los flujos en situaciones especiales como picos de demanda o apagones.\"\n",
    "                )\n",
    "\n",
    "                # Agrupar y renombrar columnas\n",
    "                df_map = df.groupby(\"primary_category\")[\"value\"].sum().reset_index()\n",
    "                df_map.columns = [\"pais_original\", \"Total\"]\n",
    "\n",
    "                # Mapeo de nombres a inglés\n",
    "                nombre_map = {\n",
    "                    \"francia\": \"France\",\n",
    "                    \"portugal\": \"Portugal\",\n",
    "                    \"andorra\": \"Andorra\",\n",
    "                    \"marruecos\": \"Morocco\"\n",
    "                }\n",
    "                df_map[\"Country\"] = df_map[\"pais_original\"].map(nombre_map)\n",
    "                df_map = df_map.dropna(subset=[\"Country\"])\n",
    "\n",
    "                # Crear diccionario país → saldo\n",
    "                country_data = df_map.set_index(\"Country\")[\"Total\"].to_dict()\n",
    "\n",
    "                # Cargar GeoJSON\n",
    "                with open(\"world_countries_with_andorra.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "                    world_geo = json.load(f)\n",
    "\n",
    "                # Crear mapa base\n",
    "                world_map = folium.Map(location=[40, 0], zoom_start=5)\n",
    "\n",
    "                # Establecer rango fijo para que el color blanco siempre sea cero\n",
    "                vmin = -8000000  # Ajusta según el rango máximo esperado de exportación\n",
    "                vmax = 8000000  # Ajusta según el rango máximo esperado de importación\n",
    "\n",
    "                colormap = cm.LinearColormap(\n",
    "                    colors=[\"blue\", \"white\", \"red\"],\n",
    "                    vmin=vmin, vmax=vmax\n",
    "                )\n",
    "\n",
    "                # Insertar saldo como propiedad en el GeoJSON\n",
    "                for feature in world_geo[\"features\"]:\n",
    "                    country_name = feature[\"properties\"][\"name\"]\n",
    "                    saldo = country_data.get(country_name)\n",
    "                    feature[\"properties\"][\"saldo\"] = saldo if saldo is not None else \"No disponible\"\n",
    "\n",
    "                # Añadir capa GeoJson con estilos y tooltips\n",
    "                folium.GeoJson(\n",
    "                    world_geo,\n",
    "                    style_function=lambda feature: {\n",
    "                        'fillColor': colormap(feature[\"properties\"][\"saldo\"])\n",
    "                        if isinstance(feature[\"properties\"][\"saldo\"], (int, float)) else 'black',\n",
    "                        'color': 'black',\n",
    "                        'weight': 1,\n",
    "                        'fillOpacity': 0.7 if isinstance(feature[\"properties\"][\"saldo\"], (int, float)) else 0.3,\n",
    "                    },\n",
    "                    tooltip=GeoJsonTooltip(\n",
    "                        fields=['name', 'saldo'],\n",
    "                        aliases=['País:', 'Saldo (MWh):'],\n",
    "                        labels=True,\n",
    "                        sticky=True,\n",
    "                        localize=True,\n",
    "                        toLocaleString=True\n",
    "                    ),\n",
    "                    highlight_function=lambda x: {'weight': 3, 'color': 'blue'}\n",
    "                ).add_to(world_map)\n",
    "\n",
    "                # Crear leyenda personalizada como MacroElement\n",
    "                legend_html = f\"\"\"\n",
    "                    {{% macro html(this, kwargs) %}}\n",
    "\n",
    "                    <div style=\"\n",
    "                        position: fixed;\n",
    "                        bottom: 50px;\n",
    "                        left: 50px;\n",
    "                        width: 250px;\n",
    "                        height: 120px;\n",
    "                        background-color: white;\n",
    "                        border:2px solid grey;\n",
    "                        z-index:9999;\n",
    "                        font-size:14px;\n",
    "                        padding: 10px;\n",
    "                        \">\n",
    "                        <b>Saldo neto de energía (MWh)</b><br><br>\n",
    "                        <div style=\"\n",
    "                            width: 200px;\n",
    "                            height: 20px;\n",
    "                            background: linear-gradient(to right, blue, white, red);\n",
    "                            border: 1px solid black;\n",
    "                        \"></div>\n",
    "                        <div style=\"display: flex; justify-content: space-between; width: 200px;\">\n",
    "                            <span style=\"font-size:12px;\">{vmin} MWh</span>\n",
    "                            <span style=\"font-size:12px;\">0</span>\n",
    "                            <span style=\"font-size:12px;\">{vmax} MWh</span>\n",
    "                        </div>\n",
    "                    </div>\n",
    "\n",
    "                    {{% endmacro %}}\n",
    "                    \"\"\"\n",
    "\n",
    "                legend = MacroElement()\n",
    "                legend._template = Template(legend_html)\n",
    "                world_map.get_root().add_child(legend)\n",
    "\n",
    "                st_folium(world_map, width=1285, height=600)\n",
    "\n",
    "                st.markdown(\n",
    "                    \"**Mapa de intercambios internacionales de energía – Contexto del apagón del 28 de abril de 2025**\\n\\n\"\n",
    "                    \"Este mapa revela cómo se comportaron los **flujos internacionales de energía** en torno al **apagón del 28 de abril de 2025**.\\n\\n\"\n",
    "                    \"Una **disminución en los intercambios con Francia o Marruecos** podría indicar una disrupción en el suministro internacional \"\n",
    "                    \"o un corte de emergencia.\\n\\n\"\n",
    "                    \"Si **España aparece como exportadora neta incluso durante el apagón**, esto sugiere que el problema no fue de generación, \"\n",
    "                    \"sino posiblemente **interno** (fallo en la red o desconexión de carga).\\n\\n\"\n",
    "                    \"La inclusión de **Andorra y Marruecos** proporciona un contexto más completo del comportamiento eléctrico en la península \"\n",
    "                    \"y el norte de África.\\n\\n\"\n",
    "                    \"Este gráfico es crucial para analizar si los intercambios internacionales actuaron de forma inusual, lo cual puede dar pistas \"\n",
    "                    \"sobre causas externas o coordinación regional durante el evento.\"\n",
    "                )\n",
    "            elif tabla == \"intercambios_baleares\":\n",
    "                # Filtramos las dos categorías\n",
    "                df_ib = df[df['primary_category'].isin(['Entradas', 'Salidas'])].copy()\n",
    "\n",
    "                # Agregamos por fecha para evitar múltiples por hora si fuera el caso\n",
    "                df_ib_grouped = df_ib.groupby(['datetime', 'primary_category'])['value'].sum().reset_index()\n",
    "\n",
    "                df_ib_grouped['value'] = df_ib_grouped['value'].abs()\n",
    "                st.markdown(\n",
    "                    \"**Intercambios de energía con Baleares (Primer semestre 2025)**\\n\\n\"\n",
    "                    \"Durante el primer semestre de **2025**, las **salidas de energía hacia Baleares** superan consistentemente a las entradas, \"\n",
    "                    \"lo que indica que el sistema peninsular actúa mayormente como **exportador neto de energía**.\\n\\n\"\n",
    "                    \"Ambos flujos muestran una **tendencia creciente hacia junio**, especialmente las salidas, lo que podría reflejar un aumento \"\n",
    "                    \"en la demanda en Baleares o una mejora en la capacidad exportadora del sistema.\"\n",
    "                )\n",
    "\n",
    "                fig = px.area(\n",
    "                    df_ib_grouped,\n",
    "                    x=\"datetime\",\n",
    "                    y=\"value\",\n",
    "                    color=\"primary_category\",\n",
    "                    labels={\"value\": \"Energía (MWh)\", \"datetime\": \"Fecha\"},\n",
    "                    title=\"Intercambios con Baleares - Área Apilada (Magnitud)\"\n",
    "                )\n",
    "\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "            else:\n",
    "                fig = px.line(df, x=\"datetime\", y=\"value\", title=\"Visualización\")\n",
    "                st.plotly_chart(fig, use_container_width=True)\n",
    "\n",
    "            with st.expander(\"Ver datos en tabla\"):\n",
    "                st.dataframe(df, use_container_width=True)\n",
    "        else:\n",
    "            st.info(\"Consulta primero los datos desde la pestaña anterior.\")\n",
    "\n",
    "    with tab4:\n",
    "        # -------------------------------------------\n",
    "        # CONFIGURACIÓN DE STREAMLIT\n",
    "        # -------------------------------------------\n",
    "        st.title(\"Predicción de Series Temporales con Modelos Preentrenados\")\n",
    "\n",
    "        # -------------------------------------------\n",
    "        # SELECCIÓN DE MODELO Y PÉRDIDA\n",
    "        # -------------------------------------------\n",
    "        model_type = st.selectbox(\"Selecciona el modelo\", [\"SimpleRNN\", \"LSTM\", \"GRU\"])\n",
    "        loss_function = st.selectbox(\"Función de pérdida\", [\"mse\", \"mae\"])\n",
    "\n",
    "        model_filename = f'models/{model_type}_model_{loss_function}.onnx'\n",
    "        scaler_filename = f'models/scaler_{model_type}_{loss_function}.pkl'\n",
    "        history_filename = f'models/{model_type}_history_{loss_function}.pkl'\n",
    "\n",
    "        try:\n",
    "            # Cargar modelo ONNX\n",
    "            session  = ort.InferenceSession(model_filename)\n",
    "\n",
    "            # Cargar scaler\n",
    "            with open(scaler_filename, 'rb') as f:\n",
    "                scaler = pickle.load(f)\n",
    "\n",
    "            # Cargar history\n",
    "            with open(history_filename, 'rb') as f:\n",
    "                history = pickle.load(f)\n",
    "\n",
    "            st.success(f\"Modelo {model_filename} cargado correctamente.\")\n",
    "\n",
    "            # -------------------------------------------\n",
    "            # GRÁFICO DE FUNCIÓN DE PÉRDIDA\n",
    "            # -------------------------------------------\n",
    "            st.subheader(\"Gráfico de la función de pérdida\")\n",
    "            df_loss = pd.DataFrame({\n",
    "                'epoch': range(1, len(history['loss']) + 1),\n",
    "                'train_loss': history['loss'],\n",
    "                'val_loss': history['val_loss']\n",
    "            })\n",
    "            fig_loss = px.line(df_loss, x='epoch', y=['train_loss', 'val_loss'],\n",
    "                               labels={'value': 'Loss', 'epoch': 'Época'},\n",
    "                               title='Evolución de la pérdida durante el entrenamiento')\n",
    "            st.plotly_chart(fig_loss, use_container_width=True)\n",
    "\n",
    "            # -------------------------------------------\n",
    "            # CARGA DE LOS DATOS\n",
    "            # -------------------------------------------\n",
    "            df_prediccion = pd.read_csv('datos_prediccion.csv')\n",
    "            df_prediccion['datetime'] = pd.to_datetime(df_prediccion['datetime'])\n",
    "            df_prediccion = df_prediccion.set_index('datetime')\n",
    "\n",
    "            df_prediccion['value_scaled'] = scaler.transform(df_prediccion[['value']])\n",
    "\n",
    "            # Preparación de datos\n",
    "            n_pasos = 24\n",
    "\n",
    "            def crear_secuencias(datos, n_pasos):\n",
    "                X, y = [], []\n",
    "                for i in range(len(datos) - n_pasos):\n",
    "                    X.append(datos[i:i + n_pasos])\n",
    "                    y.append(datos[i + n_pasos])\n",
    "                return np.array(X), np.array(y)\n",
    "\n",
    "            X, y = crear_secuencias(df_prediccion['value_scaled'].values, n_pasos)\n",
    "            X = X.reshape((X.shape[0], X.shape[1], 1)).astype('float32')\n",
    "\n",
    "            # Preparar la sesión ONNX\n",
    "            input_name = session.get_inputs()[0].name\n",
    "\n",
    "            # -------------------------------------------\n",
    "            # ONE-STEP PREDICTION\n",
    "            # -------------------------------------------\n",
    "            st.subheader(\"One-Step Prediction\")\n",
    "\n",
    "            y_pred_scaled = []\n",
    "            for i in range(X.shape[0]):\n",
    "                pred = session.run(None, {input_name: X[i:i + 1]})[0]\n",
    "                y_pred_scaled.append(pred[0][0])\n",
    "\n",
    "            y_pred_scaled = np.array(y_pred_scaled).reshape(-1, 1)\n",
    "            y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "            y_real = scaler.inverse_transform(y.reshape(-1, 1))\n",
    "\n",
    "            df_pred = pd.DataFrame({\n",
    "                'Real': y_real.flatten(),\n",
    "                'Predicción': y_pred.flatten()\n",
    "            }, index=df_prediccion.index[n_pasos:])\n",
    "\n",
    "            fig_pred = px.line(df_pred.head(200), title=\"Predicción vs Real (One-Step)\")\n",
    "            st.plotly_chart(fig_pred, use_container_width=True)\n",
    "\n",
    "            mse = mean_squared_error(y_real, y_pred)\n",
    "            st.metric(label=\"MSE (Error cuadrático medio)\", value=f\"{mse:.2f}\")\n",
    "\n",
    "            # -------------------------------------------\n",
    "            # MULTI-STEP PREDICTION\n",
    "            # -------------------------------------------\n",
    "            st.subheader(\"Multi-Step Prediction\")\n",
    "\n",
    "            # Elegir número de pasos a predecir\n",
    "            n_pred = st.slider(\"Número de pasos a predecir (Multi-Step)\", min_value=1, max_value=168, value=24, step=1)\n",
    "\n",
    "            ultimos_valores = df_prediccion['value_scaled'].values[-n_pasos:].tolist()\n",
    "            predicciones_multi = []\n",
    "\n",
    "            for _ in range(n_pred):\n",
    "                entrada = np.array(ultimos_valores[-n_pasos:]).reshape((1, n_pasos, 1)).astype('float32')\n",
    "                pred_scaled = session.run(None, {input_name: entrada})[0][0][0]\n",
    "                predicciones_multi.append(pred_scaled)\n",
    "                ultimos_valores.append(pred_scaled)\n",
    "\n",
    "            predicciones_multi = scaler.inverse_transform(np.array(predicciones_multi).reshape(-1, 1)).flatten()\n",
    "\n",
    "            fechas_futuras = pd.date_range(start=df_prediccion.index[-1] + pd.Timedelta(hours=1), periods=n_pred,\n",
    "                                           freq='H')\n",
    "\n",
    "            fig_multi = go.Figure()\n",
    "            fig_multi.add_trace(\n",
    "                go.Scatter(x=df_prediccion.index, y=df_prediccion['value'], mode='lines', name='Datos reales'))\n",
    "            fig_multi.add_trace(\n",
    "                go.Scatter(x=fechas_futuras, y=predicciones_multi, mode='lines+markers', name='Predicción Multi-Step'))\n",
    "\n",
    "            fig_multi.update_layout(title=\"Predicción Multi-Step\", xaxis_title=\"Fecha\", yaxis_title=\"Valor\")\n",
    "            st.plotly_chart(fig_multi, use_container_width=True)\n",
    "\n",
    "        except Exception as e:\n",
    "            st.warning(f\"❌ El modelo {model_type} con pérdida {loss_function} no se encuentra o ocurrió un error.\\n{e}\")\n",
    "\n",
    "    with tab5:\n",
    "        st.write(\"Modelo con Facebook Prophet\")\n",
    "        # Cargar datos de predicción\n",
    "        df_prophet = pd.read_csv('datos_prediccion_prophet.csv')\n",
    "        df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "\n",
    "        # Configuración de granularidades\n",
    "        granularidades = {\n",
    "            'Diaria': 'diaria',\n",
    "            'Semanal': 'semanal',\n",
    "            'Mensual': 'mensual',\n",
    "            'Trimestral': 'trimestral',\n",
    "            'Semestral': 'semestral',\n",
    "            'Anual': 'anual'\n",
    "        }\n",
    "\n",
    "        st.title(\"Predicciones con Prophet\")\n",
    "        granularidad_seleccionada = st.selectbox(\"Selecciona la granularidad:\", list(granularidades.keys()))\n",
    "\n",
    "        # Cargar el modelo correspondiente\n",
    "        nombre_granularidad = granularidades[granularidad_seleccionada]\n",
    "        try:\n",
    "            model_prophet = joblib.load(f'models/prophet_model_{nombre_granularidad}.joblib')\n",
    "            st.success(f\"Modelo {granularidad_seleccionada} cargado correctamente.\")\n",
    "        except Exception as e:\n",
    "            st.error(f\"No se pudo cargar el modelo para {granularidad_seleccionada}: {e}\")\n",
    "            st.stop()\n",
    "\n",
    "        # Crear fechas futuras (puedes hacer slider si quieres)\n",
    "        n_pred = st.slider(\"Número de pasos a predecir:\", min_value=10, max_value=500, value=100)\n",
    "\n",
    "        # Determinar frecuencia para futuras fechas\n",
    "        freq_map = {\n",
    "            'diaria': 'D',\n",
    "            'semanal': 'W',\n",
    "            'mensual': 'M',\n",
    "            'trimestral': 'Q',\n",
    "            'semestral': '2Q',\n",
    "            'anual': 'A'\n",
    "        }\n",
    "        freq = freq_map[nombre_granularidad]\n",
    "\n",
    "        # Preparar datos para predicción futura\n",
    "        future = model_prophet.make_future_dataframe(periods=n_pred, freq=freq)\n",
    "\n",
    "        # Realizar predicción\n",
    "        forecast = model_prophet.predict(future)\n",
    "\n",
    "        # Mostrar gráfica de predicción\n",
    "        st.subheader(\"Predicción\")\n",
    "        fig1 = plot_plotly(model_prophet, forecast)\n",
    "        st.plotly_chart(fig1, use_container_width=True)\n",
    "\n",
    "        # Mostrar componentes\n",
    "        st.subheader(\"Componentes de la predicción\")\n",
    "        fig2 = plot_components_plotly(model_prophet, forecast)\n",
    "        st.plotly_chart(fig2, use_container_width=True)\n",
    "\n",
    "    with tab6:\n",
    "        if tabla == \"demanda\":\n",
    "\n",
    "            # --- HEATMAP ---\n",
    "            df_heatmap = df.copy()\n",
    "            df_heatmap['weekday'] = df_heatmap['datetime'].dt.day_name()\n",
    "            df_heatmap['hour'] = df_heatmap['datetime'].dt.hour\n",
    "\n",
    "            days_order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "\n",
    "            heatmap_data = (\n",
    "                df_heatmap.groupby(['weekday', 'hour'])['value']\n",
    "                .mean()\n",
    "                .reset_index()\n",
    "                .pivot(index='weekday', columns='hour', values='value')\n",
    "                .reindex(days_order)\n",
    "            )\n",
    "            st.markdown(\n",
    "                \"**Demanda promedio por día y hora**\\n\\n\"\n",
    "                \"La demanda eléctrica promedio es más alta entre semana, especialmente de **lunes a viernes**, \"\n",
    "                \"con picos concentrados entre las **7:00 y 21:00 horas**. El máximo se registra los **viernes alrededor de las 19:00 h**, \"\n",
    "                \"superando los **32 000 MW**.\\n\\n\"\n",
    "                \"En contraste, los **fines de semana** muestran una demanda notablemente más baja y estable.\"\n",
    "            )\n",
    "            fig1 = px.imshow(\n",
    "                heatmap_data,\n",
    "                labels=dict(x=\"Hora del día\", y=\"Día de la semana\", color=\"Demanda promedio (MW)\"),\n",
    "                x=heatmap_data.columns,\n",
    "                y=heatmap_data.index,\n",
    "                color_continuous_scale=\"YlGnBu\",\n",
    "                aspect=\"auto\",\n",
    "            )\n",
    "            fig1.update_layout(title=\"Demanda promedio por día y hora\")\n",
    "\n",
    "            st.plotly_chart(fig1, use_container_width=True)\n",
    "\n",
    "            # --- BOXPLOT ---\n",
    "            df_box = df.copy()\n",
    "\n",
    "            df_box[\"month\"] = df_box[\"datetime\"].dt.month\n",
    "            st.markdown(\n",
    "                \"**Distribución de Demanda por mes (2025)**\\n\\n\"\n",
    "                \"La demanda eléctrica presenta **mayor variabilidad y valores más altos en los primeros tres meses del año**, \"\n",
    "                \"especialmente en **enero**.\\n\\n\"\n",
    "                \"En **abril**, se observa una mayor cantidad de valores atípicos a la baja, lo cual coincide con el \"\n",
    "                \"**apagón nacional del 28/04/2025**, donde España estuvo sin luz durante aproximadamente 8 a 10 horas.\\n\\n\"\n",
    "                \"A partir de **mayo**, la demanda se estabiliza ligeramente, con una reducción progresiva en la mediana mensual.\"\n",
    "            )\n",
    "            fig2 = px.box(\n",
    "                df_box,\n",
    "                x=\"month\",\n",
    "                y=\"value\",\n",
    "                title=\"Distribución de Demanda por mes\",\n",
    "                labels={\"value\": \"Demanda (MWh)\", \"hour\": \"Hora del Día\"}\n",
    "            )\n",
    "\n",
    "            st.plotly_chart(fig2, use_container_width=True)\n",
    "\n",
    "        else:\n",
    "            st.markdown(\"Nada que ver... de momento\")\n",
    "\n",
    "    with tab7:  #CAMBIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "    mostrar_estructura_base_datos()\n",
    "\n",
    "\n",
    "    with tab8: #CAMBIOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOOO\n",
    "        st.subheader(\"Sobre Nosotros \")\n",
    "\n",
    "        st.markdown(\"\"\"\n",
    "        Este proyecto ha sido desarrollado como parte del Sprint II del Bootcamp de Ciencia de Datos.\n",
    "\n",
    "        ####  Adrián Acedo Quintanar\n",
    "        - Rol:Facilitador\n",
    "        - GitHub: [Adrián Acedo](https://github.com/AdrianAcedo)\n",
    "        - LinkedIn: [Adrián Acedo](https://www.linkedin.com/in/adrianacedoquintanar/)\n",
    "\n",
    "        ####  Lucía Varela\n",
    "        - Rol: \n",
    "        - GitHub: *(Enlace no disponible)*\n",
    "        - LinkedIn: *(Enlace no disponible)*\n",
    "\n",
    "        ####  Génesis\n",
    "        - Rol: \n",
    "        - GitHub: *(Enlace no disponible)*\n",
    "        - LinkedIn: *(Enlace no disponible)*\n",
    "\n",
    "        ---\n",
    "        Si deseas saber más sobre nuestro trabajo, contáctanos por LinkedIn o consulta el repositorio de GitHub.\n",
    "        \"\"\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea7f91c-ce7c-4b51-b43c-2181a30c5544",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070b19a-adbb-4f52-b7fd-48b970f3d29a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "497c3fc2-cf6e-4ca7-80f6-307e487bf29c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432ad99d-fec8-429b-9e35-b92cef6da946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6825f4b-2d36-4275-ab17-e4929169bf87",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568c2dc0-5446-469b-a65a-e11e4c0b333f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53289300-08ec-4fe3-a833-471168c2c19b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119c3e5-8686-46a9-b6e8-167875141717",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84522023-0474-4d21-bd34-b16becc74f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40a67818-1e70-44f2-af6e-3f2fb63b3f1f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ab617b-ca61-498f-8d86-74b7a810471e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c39449-75db-4479-9471-628cfa841b78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96bc422f-dc81-497c-866c-2e2e0f8c8982",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
