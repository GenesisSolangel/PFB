{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install supabase tf2onnx prophet joblib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import SimpleRNN, LSTM, GRU, Dense\nfrom sklearn.preprocessing import MinMaxScaler\nimport pickle\nfrom supabase import create_client, Client\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta, timezone\nimport plotly.express as px\nimport tensorflow as tf\nimport tf2onnx\nfrom prophet import Prophet\nimport joblib","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# FUNCION DE DESCARGA DE DATOS \ndef get_demanda_data(page_size=1000):\n    end = datetime.now(timezone.utc)\n    start = end - timedelta(days=3650)\n    all_data = []\n    offset = 0\n    while True:\n        response = (\n            supabase.table(\"demanda\")\n            .select(\"datetime,value\")\n            .gte(\"datetime\", start.isoformat())\n            .lte(\"datetime\", end.isoformat())\n            .range(offset, offset + page_size - 1)\n            .execute()\n        )\n        data = response.data\n        if not data:\n            break\n        all_data.extend(data)\n        offset += page_size\n        if len(data) < page_size:\n            break\n\n    if not all_data:\n        return pd.DataFrame()\n    df = pd.DataFrame(all_data)\n    df[\"datetime\"] = pd.to_datetime(df[\"datetime\"])\n    return df\n\n# FUNCION DE ESCALADO Y GUARDADO DEL SCALER\ndef escalar_datos(df, save_path='scaler.pkl'):\n    scaler = MinMaxScaler()\n    df['value_scaled'] = scaler.fit_transform(df[['value']])\n\n    with open(save_path, 'wb') as f:\n        pickle.dump(scaler, f)\n\n    print(f\" Datos escalados y scaler guardado en '{save_path}'.\")\n    return df, scaler\n\n# FUNCION PARA DIVIDIR EN TRAIN Y TEST\ndef dividir_train_test(df, test_size=0.2):\n    df = df.copy()\n    df = df.set_index('datetime')\n    split_point = int(len(df) * (1 - test_size))\n    df_train = df.iloc[:split_point]\n    df_test = df.iloc[split_point:]\n    return df_train, df_test\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cargar datos\ndf = get_demanda_data()\ndf, scaler = escalar_datos(df)\n# Guardar dataset como CSV para usarlo en Streamlit\ndf.to_csv('datos_prediccion.csv', index=False)\nprint(\"âœ… Dataset guardado como 'datos_prediccion.csv'\")\ndf_train, df_test = dividir_train_test(df)\n\ndef crear_secuencias(datos, n_pasos):\n    X, y = [], []\n    for i in range(len(datos) - n_pasos):\n        X.append(datos[i:i + n_pasos])\n        y.append(datos[i + n_pasos])\n    return np.array(X), np.array(y)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ConfiguraciÃ³n de hiperparÃ¡metros\nmodel_type = 'SimpleRNN'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mse'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 40\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # âš™ï¸ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el scaler\n#model.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\n# Guardar el modelo en ONNX\n# 1. Crear el input_signature\ninput_signature = [tf.TensorSpec([None, n_pasos, 1], tf.float32, name=\"input\")]\n\n# 2. Decorar la funciÃ³n (no obtener concrete_func)\n@tf.function(input_signature=input_signature)\ndef model_func(x):\n    return model(x)\n\n# 3. Convertir a ONNX directamente desde la funciÃ³n decorada\nonnx_model, _ = tf2onnx.convert.from_function(\n    model_func,  # ðŸ‘‰ AquÃ­ debes pasar la funciÃ³n decorada, no la concrete_func\n    input_signature=input_signature,\n    opset=13,\n    output_path=f'{model_type}_model_{loss_function}.onnx'\n)\n\nprint(f'âœ… Modelo {model_type}_model_{loss_function}.onnx entrenado y guardado correctamente.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# GrÃ¡fico de pÃ©rdida\nfig_loss = px.line(y=history.history['loss'], title='PÃ©rdida durante el entrenamiento')\nfig_loss.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ConfiguraciÃ³n de hiperparÃ¡metros\nmodel_type = 'LSTM'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mse'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 40\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # âš™ï¸ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el scaler\n#model.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\n# Guardar el modelo en ONNX\n# 1. Crear el input_signature\ninput_signature = [tf.TensorSpec([None, n_pasos, 1], tf.float32, name=\"input\")]\n\n# 2. Decorar la funciÃ³n (no obtener concrete_func)\n@tf.function(input_signature=input_signature)\ndef model_func(x):\n    return model(x)\n\n# 3. Convertir a ONNX directamente desde la funciÃ³n decorada\nonnx_model, _ = tf2onnx.convert.from_function(\n    model_func,  # ðŸ‘‰ AquÃ­ debes pasar la funciÃ³n decorada, no la concrete_func\n    input_signature=input_signature,\n    opset=13,\n    output_path=f'{model_type}_model_{loss_function}.onnx'\n)\n\nprint(f'âœ… Modelo {model_type}_model_{loss_function}.onnx entrenado y guardado correctamente.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='PÃ©rdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ConfiguraciÃ³n de hiperparÃ¡metros\nmodel_type = 'GRU'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mse'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 40\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # âš™ï¸ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el scaler\n#model.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\n# Guardar el modelo en ONNX\n# 1. Crear el input_signature\ninput_signature = [tf.TensorSpec([None, n_pasos, 1], tf.float32, name=\"input\")]\n\n# 2. Decorar la funciÃ³n (no obtener concrete_func)\n@tf.function(input_signature=input_signature)\ndef model_func(x):\n    return model(x)\n\n# 3. Convertir a ONNX directamente desde la funciÃ³n decorada\nonnx_model, _ = tf2onnx.convert.from_function(\n    model_func,  # ðŸ‘‰ AquÃ­ debes pasar la funciÃ³n decorada, no la concrete_func\n    input_signature=input_signature,\n    opset=13,\n    output_path=f'{model_type}_model_{loss_function}.onnx'\n)\n\nprint(f'âœ… Modelo {model_type}_model_{loss_function}.onnx entrenado y guardado correctamente.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='PÃ©rdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ConfiguraciÃ³n de hiperparÃ¡metros\nmodel_type = 'SimpleRNN'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mae'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 50\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # âš™ï¸ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el scaler\n#model.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\n# Guardar el modelo en ONNX\n# 1. Crear el input_signature\ninput_signature = [tf.TensorSpec([None, n_pasos, 1], tf.float32, name=\"input\")]\n\n# 2. Decorar la funciÃ³n (no obtener concrete_func)\n@tf.function(input_signature=input_signature)\ndef model_func(x):\n    return model(x)\n\n# 3. Convertir a ONNX directamente desde la funciÃ³n decorada\nonnx_model, _ = tf2onnx.convert.from_function(\n    model_func,  # ðŸ‘‰ AquÃ­ debes pasar la funciÃ³n decorada, no la concrete_func\n    input_signature=input_signature,\n    opset=13,\n    output_path=f'{model_type}_model_{loss_function}.onnx'\n)\n\nprint(f'âœ… Modelo {model_type}_model_{loss_function}.onnx entrenado y guardado correctamente.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='PÃ©rdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ConfiguraciÃ³n de hiperparÃ¡metros\nmodel_type = 'LSTM'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mae'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 50\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # âš™ï¸ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el scaler\n#model.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\n# Guardar el modelo en ONNX\n# 1. Crear el input_signature\ninput_signature = [tf.TensorSpec([None, n_pasos, 1], tf.float32, name=\"input\")]\n\n# 2. Decorar la funciÃ³n (no obtener concrete_func)\n@tf.function(input_signature=input_signature)\ndef model_func(x):\n    return model(x)\n\n# 3. Convertir a ONNX directamente desde la funciÃ³n decorada\nonnx_model, _ = tf2onnx.convert.from_function(\n    model_func,  # ðŸ‘‰ AquÃ­ debes pasar la funciÃ³n decorada, no la concrete_func\n    input_signature=input_signature,\n    opset=13,\n    output_path=f'{model_type}_model_{loss_function}.onnx'\n)\n\nprint(f'âœ… Modelo {model_type}_model_{loss_function}.onnx entrenado y guardado correctamente.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='PÃ©rdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ConfiguraciÃ³n de hiperparÃ¡metros\nmodel_type = 'GRU'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mae'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 50\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # âš™ï¸ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el scaler\n#model.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\n# Guardar el modelo en ONNX\n# 1. Crear el input_signature\ninput_signature = [tf.TensorSpec([None, n_pasos, 1], tf.float32, name=\"input\")]\n\n# 2. Decorar la funciÃ³n (no obtener concrete_func)\n@tf.function(input_signature=input_signature)\ndef model_func(x):\n    return model(x)\n\n# 3. Convertir a ONNX directamente desde la funciÃ³n decorada\nonnx_model, _ = tf2onnx.convert.from_function(\n    model_func,  # ðŸ‘‰ AquÃ­ debes pasar la funciÃ³n decorada, no la concrete_func\n    input_signature=input_signature,\n    opset=13,\n    output_path=f'{model_type}_model_{loss_function}.onnx'\n)\n\nprint(f'âœ… Modelo {model_type}_model_{loss_function}.onnx entrenado y guardado correctamente.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='PÃ©rdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Cargar datos para Prophet\ndf_prophet = get_demanda_data()\ndf_prophet['datetime'] = df_prophet['datetime'].dt.tz_localize(None)\ndf_prophet = df_prophet.rename(columns={\"datetime\": \"ds\", \"value\": \"y\"})","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Guardar en csv \ndf_prophet.to_csv('datos_prediccion_prophet.csv', index=False)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Definir granularidades\ngranularidades = {\n    'D': 'diaria',\n    'W': 'semanal',\n    'ME': 'mensual',\n    'QE': 'trimestral',\n    '2Q': 'semestral',\n    'YE': 'anual'\n}\n\n# Horizontes de predicciÃ³n\nhorizontes = [10, 50, 100]\n\nfor freq, nombre in granularidades.items():\n    print(f'Entrenando modelo para granularidad: {nombre}')\n    \n    # Reagrupar datos segÃºn granularidad\n    df_freq = df_prophet.copy()\n    df_freq = df_freq.groupby(pd.Grouper(key='ds', freq=freq)).sum().reset_index()\n\n    model_prophet = Prophet()\n    model_prophet.fit(df_freq)\n\n    # Guardar modelo (por si quieres seguir usando mÃ¡s adelante)\n    joblib.dump(model_prophet, f'prophet_model_{nombre}.joblib')\n\n    # Precalcular predicciones para diferentes horizontes\n    for n_pred in horizontes:\n        print(f'Generando predicciÃ³n para {n_pred} pasos')\n\n        future = model_prophet.make_future_dataframe(periods=n_pred, freq=freq)\n        forecast = model_prophet.predict(future)\n\n        # Guardar predicciÃ³n\n        forecast.to_csv(f'forecast_{nombre}_{n_pred}.csv', index=False)\n\n    print(f'âœ… Modelo Prophet {nombre} y predicciones guardados correctamente.')\n\nprint('ðŸŽ‰ Todos los modelos y predicciones entrenados y guardados.')","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}