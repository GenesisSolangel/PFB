{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from keras.models import Sequential\nfrom keras.layers import SimpleRNN, LSTM, GRU, Dense\nfrom sklearn.preprocessing import MinMaxScaler\nimport pickle\nfrom supabase import create_client, Client\nimport pandas as pd\nimport numpy as np\nfrom datetime import datetime, timedelta, timezone\nimport plotly.express as px","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T15:04:33.736019Z","iopub.execute_input":"2025-06-17T15:04:33.736453Z","iopub.status.idle":"2025-06-17T15:04:33.742284Z","shell.execute_reply.started":"2025-06-17T15:04:33.736415Z","shell.execute_reply":"2025-06-17T15:04:33.741402Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"SUPABASE_URL = \"https://gbfxqkzjzamqlqhzvbqc.supabase.co\"\nSUPABASE_KEY = \"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImdiZnhxa3pqemFtcWxxaHp2YnFjIiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDkwMjc3MDksImV4cCI6MjA2NDYwMzcwOX0.ju_muEo9aTGT8FWFYpP-5_uEaywdSn7xOPllt1VQtUQ\"\nsupabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:28:25.600272Z","iopub.execute_input":"2025-06-17T02:28:25.600710Z","iopub.status.idle":"2025-06-17T02:28:25.751157Z","shell.execute_reply.started":"2025-06-17T02:28:25.600677Z","shell.execute_reply":"2025-06-17T02:28:25.749980Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# FUNCION DE DESCARGA DE DATOS \ndef get_demanda_data():\n    end = datetime.now(timezone.utc)\n    start = end - timedelta(days=3650)\n    response = (\n        supabase.table(\"demanda\")\n        .select(\"datetime,value\")\n        .gte(\"datetime\", start.isoformat())\n        .lte(\"datetime\", end.isoformat())\n        .execute()\n    )\n    df = pd.DataFrame(response.data)\n    df['datetime'] = pd.to_datetime(df['datetime'])\n    df.sort_values('datetime', inplace=True)\n    return df\n\n# FUNCION DE ESCALADO Y GUARDADO DEL SCALER\ndef escalar_datos(df, save_path='scaler.pkl'):\n    scaler = MinMaxScaler()\n    df['value_scaled'] = scaler.fit_transform(df[['value']])\n\n    with open(save_path, 'wb') as f:\n        pickle.dump(scaler, f)\n\n    print(f\" Datos escalados y scaler guardado en '{save_path}'.\")\n    return df, scaler\n\n# FUNCION PARA DIVIDIR EN TRAIN Y TEST\ndef dividir_train_test(df, test_size=0.2):\n    df = df.copy()\n    df = df.set_index('datetime')\n    split_point = int(len(df) * (1 - test_size))\n    df_train = df.iloc[:split_point]\n    df_test = df.iloc[split_point:]\n    return df_train, df_test\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:28:25.752441Z","iopub.execute_input":"2025-06-17T02:28:25.752783Z","iopub.status.idle":"2025-06-17T02:28:25.763465Z","shell.execute_reply.started":"2025-06-17T02:28:25.752745Z","shell.execute_reply":"2025-06-17T02:28:25.762131Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Cargar datos\ndf = get_demanda_data()\ndf, scaler = escalar_datos(df)\n# Guardar dataset como CSV para usarlo en Streamlit\ndf.to_csv('datos_prediccion.csv', index=False)\nprint(\"✅ Dataset guardado como 'datos_prediccion.csv'\")\ndf_train, df_test = dividir_train_test(df)\n\ndef crear_secuencias(datos, n_pasos):\n    X, y = [], []\n    for i in range(len(datos) - n_pasos):\n        X.append(datos[i:i + n_pasos])\n        y.append(datos[i + n_pasos])\n    return np.array(X), np.array(y)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-17T02:28:27.499675Z","iopub.execute_input":"2025-06-17T02:28:27.500130Z","iopub.status.idle":"2025-06-17T02:28:28.071442Z","shell.execute_reply.started":"2025-06-17T02:28:27.500100Z","shell.execute_reply":"2025-06-17T02:28:28.070358Z"}},"outputs":[{"name":"stdout","text":" Datos escalados y scaler guardado en 'scaler.pkl'.\n✅ Dataset guardado como 'datos_prediccion.csv'\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"# Configuración de hiperparámetros\nmodel_type = 'SimpleRNN'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mse'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 40\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # ⚙️ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el modelo y el scaler\nmodel.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n    \n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\nprint(f'Modelo {model_type} entrenado y guardado.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Gráfico de pérdida\nfig_loss = px.line(y=history.history['loss'], title='Pérdida durante el entrenamiento')\nfig_loss.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuración de hiperparámetros\nmodel_type = 'LSTM'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mse'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 40\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # ⚙️ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el modelo y el scaler\nmodel.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\nprint(f'Modelo {model_type} entrenado y guardado.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='Pérdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuración de hiperparámetros\nmodel_type = 'GRU'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mse'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 40\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # ⚙️ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el modelo y el scaler\nmodel.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\nprint(f'Modelo {model_type} entrenado y guardado.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='Pérdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuración de hiperparámetros\nmodel_type = 'SimpleRNN'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mae'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 50\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # ⚙️ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el modelo y el scaler\nmodel.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\nprint(f'Modelo {model_type} entrenado y guardado.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='Pérdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuración de hiperparámetros\nmodel_type = 'LSTM'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mae'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 50\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # ⚙️ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el modelo y el scaler\nmodel.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\nprint(f'Modelo {model_type} entrenado y guardado.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='Pérdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Configuración de hiperparámetros\nmodel_type = 'GRU'  # Cambia por 'SimpleRNN', 'LSTM' o 'GRU' para entrenar otros modelos\nloss_function = 'mae'  # Puedes cambiar por 'mse' o 'mae'\nn_pasos = 24\nn_epochs = 50\nbatch_size = 32\nunidades = 50\n\n# Crear secuencias\nX_train, y_train = crear_secuencias(df_train['value_scaled'].values, n_pasos)\nX_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n\n# Crear secuencias para Test\nX_test, y_test = crear_secuencias(df_test['value_scaled'].values, n_pasos)\nX_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n\n# Definir el modelo\nif model_type == 'SimpleRNN':\n    rnn_layer = SimpleRNN(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'LSTM':\n    rnn_layer = LSTM(unidades, activation='tanh', input_shape=(n_pasos, 1))\nelif model_type == 'GRU':\n    rnn_layer = GRU(unidades, activation='tanh', input_shape=(n_pasos, 1))\n\nmodel = Sequential([\n    rnn_layer,\n    Dense(1)\n])\n\nmodel.compile(optimizer='adam', loss=loss_function)\n\n# Entrenamiento\nhistory = model.fit(\n    X_train, y_train,\n    epochs=n_epochs,\n    batch_size=batch_size,\n    validation_data=(X_test, y_test),  # ⚙️ Esto es obligatorio para tener val_loss\n    verbose=1\n)\n\n# Guardar el modelo y el scaler\nmodel.save(f'{model_type}_model_{loss_function}.keras')\n\nwith open(f'scaler_{model_type}_{loss_function}.pkl', 'wb') as f:\n    pickle.dump(scaler, f)\n\n# Guardar el history\nwith open(f\"{model_type}_history_{loss_function}.pkl\", 'wb') as f:\n    pickle.dump(history.history, f)\n\nprint(f'Modelo {model_type} entrenado y guardado.')\nprint(f'Scaler {model_type} guardado.')\nprint(f\"Historial de entrenamiento {model_type} guardado.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"px.line(y=history.history['loss'], title='Pérdida durante el entrenamiento').show()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}
